{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4ed37f1-b9ce-43b9-ab69-639696385394",
   "metadata": {},
   "source": [
    "# Outomation RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5e8dde-a751-4c40-92aa-49f5c30afe41",
   "metadata": {},
   "source": [
    "This notebook contains the final RAG pipeline made for Outomation AI Workfow Automation Externship. Goal of this RAG pipeline is to provide a robust way to handle Mortgage documents obtained from clients and assist the user in processing the home mortgage applications while answering another relevant queries from the documents. We will also create a Gradio chat interface allowing users to access the RAG pipeline running on a local machine.\n",
    "\n",
    "For this project we will be using offline open source LLM. We decided on llama3.1-8b-Q8 which performed better than many others we could use on our local hardware, we also tested quantized version of minstral7b, openai_neo, deepseek_r1, and few others before deciding. But even then, our testing of pipeline with Gemini API gave us much better and coherent result. Better LLM in backend will improve the quality of response and performance of the RAG Pipeline overall.\n",
    "\n",
    "To design the structure of the RAG Pipeline we used Mortgage Documents provided by Outomation, documents contained scanned documents and blob documents, but all files will be in PDF form. In this notebook, we will apply the RAG pipeline on the test documents which were not referred while making the pipeline. We will also perform field accuracy analysis, recall analysis, and latency analysis. \n",
    "\n",
    "Following steps were taken to create this RAG Pipeline:\n",
    "\n",
    "1. First we added ability to read all PDF files in a directory and extract text.\n",
    "2. Added ability to handle Scanned PDFs by having tesseract fallback if no text is extracted from the page.\n",
    "3. Added ability to handle blob files, multiple PDF documents packaged as one.\n",
    "4. We also make use of Logical Document splitting, allowing RAG pipeline to decide the document boundaries. Better LLM in backend would give much better document classification and boundary. We recommend using AWS or Azure APIs to do these tasks and run other parts of RAG pipeline on local LLM. \n",
    "5. Used Recursive Splitting to create document chunks and then use Semantic Chunking to create semantic nodes or chunk for Vector Store Index.\n",
    "6. Used Fiass Vector store for better Nearest Neighbor searching of chunks or nodes.\n",
    "7. Then we designed a custom retriever which is using Hybrid Retriever and Query expansion while using Metadata filtering so that only doc types related to user query appear as source nodes, this was done while adding a graceful fallback in case LLM is not able to suggest the document type properly.\n",
    "8. Finally we Cross Encoder Reranker and Strict Output Controls to finetune the final node retrievel and response.\n",
    "\n",
    "Then on the basis of this RAG Pipeline we created a Gradio App which can streamline whole process through an online app with application running on local machine. \n",
    "\n",
    "This project will be divided into two parts:\n",
    "1. RAG Pipeline\n",
    "2. Gradio App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b296ef3c-8821-482b-affe-b492fbbd64b1",
   "metadata": {},
   "source": [
    "<b> Note:</b> Gradio App was made on different notebook, but we were needed to send one single notebook so we combined them into one. In gradio app section a lot of code used in RAG pipeline has been reused, this could allow users to take the code in that section and use is as a python script as its indpendent to overall RAG pipeline section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9a958a-936e-4c4b-b86b-507e8c8a2a54",
   "metadata": {},
   "source": [
    "## RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36245f99-26d0-4694-ba53-419dafe86d27",
   "metadata": {},
   "source": [
    "In this section we will create and test a RAG pipeline which follows industry‑standard RAG pattern for grounding LLM outputs in enterprise data, and follows best‑practice retrieval steps (hybrid search, reranking, and prompt discipline) that improve accuracy and trust.\n",
    "\n",
    "We will use FAISS vector store which is highly scalable compared to default vector store so that this RAG pipeline could be scaled to 1000s of docuemnts or even more with slight modifications. We will apply our RAG architecure on text documents which contain more type of documents than we designed this RAG pipeline for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01ff0eb-79a5-40a2-9c3b-fb8d2cecd6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing essential llama index libraries and gradio, we will import others later when they are used\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex, Document\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.readers.file import PDFReader\n",
    "from llama_cpp import Llama\n",
    "import gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e5b625-8444-4de2-ad30-46f2fb04908f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get('CONDA_DEFAULT_ENV'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e14caeeb-f6c2-46ce-8a54-ab0fa7087716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e9a9b5c-ae23-4ac2-ba3b-8b83766cd739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What is the capital of France?\n",
      "Paris is the capital of France. It is located in the northern part of the country, on the Seine River. Paris\n"
     ]
    }
   ],
   "source": [
    "#loading llama 3.1-8b Q8 to be used for llm related tasks\n",
    "\n",
    "llm = Llama(\n",
    "    model_path = 'llama_3.1_8b_q8.gguf',\n",
    "    n_ctx = 4096,\n",
    "    n_gpu_layers = -1,          \n",
    "    n_batch = 256,\n",
    "    verbose = False,\n",
    "    temperature = 0.2\n",
    ")\n",
    "output = llm(\"What is the capital of France?\", max_tokens=32) #testing the model\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e13f1c43-1807-422d-88d0-bc323b92d4d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", HRN\n",
      "MORTGAGE DOC.# 10009588\n",
      "\n",
      "DOCUMENT NUMBER\n",
      "\n",
      "RECORDED 06/28/2011 09:35AM\n",
      "JOHN LA FAVE\n",
      "NAME & RETURN ADDRESS REGISTER OF DEEDS\n",
      "M&I Home Lending Solutions Milwaukee County, WI|\n",
      "Attn: Secondary Marketing AMOUNT: on 00\n",
      "\n",
      "4121 NW Urbandale Drive\n",
      "\n",
      "Urbandale, IA 50322 FEE EXEMPT #:\n",
      "\n",
      "PARCEL IDENTIFIER NUMBER\n",
      "716-0027-6\n",
      "\n",
      "[Space Above This Line For Recording Data]\n",
      "\n",
      "FHA Case No\n",
      "State of Wisconsin\n",
      "581-4247085-703 :\n",
      "\n",
      "MIN 100273100009309945\n",
      "\n",
      "THIS MORTGAGE (\"Security Instrument\") is given on June 20, 2011\n",
      "Th\n",
      "assigns) and to the successors and assigns of MERS, with power of sale, the following described property located in\n",
      "MILWAUKEE County, Wisconsin\n",
      "LOT 27, IN BLOCK 1, IN MILWAUKEE COLLEGE HEIGHTS, BEING A SUBDIVISION OF A PART\n",
      "\n",
      "OF THE EAST 1/2 OF SECTION 6, IN TOWNSHIP 5 NORTH, RANGE 22 EAST, IN THE CITY OF\n",
      "MILWAUKEE, COUNTY OF MILWAUKEE, STATE OF WISCONSIN.\n",
      "\n",
      "which has the address of 6468 SOUTH 20TH STREET [Strect]\n",
      "MILWAUKEE [City], Wisconsin 53221 [Zip Code] (\"Property Address\"),\n",
      "\n",
      "TOGETHER WITH al\n",
      "BY SIGNING BELOW, Borrower accepts and agrees to the terms contained in this Security Instrument and in\n",
      "any rider(s) executed by Borrower and recorded with it\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Witnesses\n",
      "\n",
      "Hunnbale, Ubon (Seal)\n",
      "KIMBERLY HOGAN -Borrower\n",
      "(Seal)\n",
      "-Borrower\n",
      "(Seal) (Seal)\n",
      "-Borrower -Borrower\n",
      "(Seal) (Seal)\n",
      "-Borrower -Borrower\n",
      "(Seal) (Seal)\n",
      "-Borrower -Borrower\n",
      "LN: **0994\n",
      "\n",
      "VMP®-4N(WI) (0305) 02 Page 8 of 9\n",
      "\n",
      "Doc Yr: 2011 Doc# 10009588 Page# 8 of 9\n",
      "\f",
      "\n",
      "STATE OF WISCONSIN, MN futuleey County ss:\n",
      "\n",
      "The foregoing instrument was acknowledged before me this Gene Ag LO/[\n",
      "\n",
      "by KIMBERLY HOGAN\n",
      "\n",
      "My Commission Expires\n",
      "\n",
      "Notary Public, State of Wisconsin\n",
      "¢ 7\n",
      "\n",
      "f a A] / This instrument was prepared by\n",
      "\n",
      "Kelly Bo Zylstra\n",
      "\n",
      "4121 NW Urbandale Drive\n",
      "\n",
      "Urbandale, IA 50322\n",
      "\n",
      "RONDELA E EMRICK\n",
      "NOTARY PUBLIC\n",
      "\n",
      "STATE OF WISCONSIN\n",
      "\n",
      " \n",
      "\n",
      "code 27\n",
      "\n",
      "N:**9994\n",
      "use KEE\"\n",
      "\n",
      "VMP®-4N(WI) (0305) 02 Page 9 of 9\n",
      "\n",
      "Doc Yr: 2011 Doc# 10009588 Page# 9 of 9\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "#testing a scanned document text extraction, this document is from training dataset\n",
    "from PIL import Image, ImageOps\n",
    "import pytesseract\n",
    "import os, fitz, re\n",
    "\n",
    "doc = fitz.open('MTG_10009588.pdf')\n",
    "for page in doc:\n",
    "    if page.get_text:\n",
    "        img = page.get_pixmap(dpi = 300)\n",
    "        img_b = img.tobytes('png')\n",
    "        img = Image.open(io.BytesIO(img_b))\n",
    "        text = pytesseract.image_to_string(img)\n",
    "        print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2059314-ceae-464a-97b9-faa370588e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "import pytesseract\n",
    "import camelot\n",
    "\n",
    "from llama_index.core import Document\n",
    "\n",
    "CURRENCY_RE = re.compile(r'[$€₹£]\\s?\\d')\n",
    "NUMBER_RE   = re.compile(r'\\d')\n",
    "DATE_RE     = re.compile(r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b', re.I)\n",
    "STOP_PHRASES = [\n",
    "    \"thank you\", \"we appreciate your business\", \"page\", \"fax\", \"phone\", \"email\",\n",
    "    \"powered by\", \"this is not a\", \"confidential\", \"copyright\"\n",
    "]\n",
    "\n",
    "def _has_ruled_lines(page: fitz.Page, min_lines: int = 8) -> bool:\n",
    "    \"\"\"Detect presence of many straight lines (rulings) on the page.\"\"\"\n",
    "    try:\n",
    "        drawings = page.get_drawings()\n",
    "        lines = 0\n",
    "        for d in drawings:\n",
    "            for p in d.get(\"items\", []):\n",
    "                if p[0] == \"l\":  # line\n",
    "                    lines += 1\n",
    "        return lines >= min_lines\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _row_informative(cells: List[str], min_chars: int = 10) -> bool:\n",
    "    s = \" \".join(cells).strip()\n",
    "    if len(s) < min_chars:\n",
    "        return False\n",
    "    low = s.lower()\n",
    "    if any(ph in low for ph in STOP_PHRASES):\n",
    "        return False\n",
    "    # require some signal: number, currency, or date\n",
    "    if CURRENCY_RE.search(s) or DATE_RE.search(s) or NUMBER_RE.search(s):\n",
    "        return True\n",
    "    # allow longer textual rows (e.g., purpose sentences)\n",
    "    return len(s) >= (min_chars * 2)\n",
    "\n",
    "class MixedPDFReader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dpi: int = 300,\n",
    "        lang: str = \"eng\",\n",
    "        min_chars: int = 40,\n",
    "        ocr_psm: int = 6,\n",
    "        extract_tables: bool = True,\n",
    "        allow_stream: bool = False,           # default off to avoid false positives\n",
    "        min_table_rows: int = 3,\n",
    "        min_table_cols: int = 3,\n",
    "        max_rows_per_table: int = 60,\n",
    "        max_nodes_per_page: int = 120,        # hard cap per page\n",
    "        emit_table_rows: bool = True,         # False => emit whole-table docs only\n",
    "    ):\n",
    "        self.dpi = dpi\n",
    "        self.lang = lang\n",
    "        self.min_chars = min_chars\n",
    "        self.ocr_psm = ocr_psm\n",
    "        self.extract_tables = extract_tables\n",
    "        self.allow_stream = allow_stream\n",
    "        self.min_table_rows = min_table_rows\n",
    "        self.min_table_cols = min_table_cols\n",
    "        self.max_rows_per_table = max_rows_per_table\n",
    "        self.max_nodes_per_page = max_nodes_per_page\n",
    "        self.emit_table_rows = emit_table_rows\n",
    "\n",
    "    def _has_meaningful_text(self, page: fitz.Page) -> bool:\n",
    "        txt = page.get_text(\"text\") or \"\"\n",
    "        return len(txt.strip()) >= self.min_chars\n",
    "\n",
    "    def _page_to_image(self, page: fitz.Page) -> Image.Image:\n",
    "        scale = self.dpi / 72.0\n",
    "        pm = page.get_pixmap(matrix=fitz.Matrix(scale, scale))\n",
    "        mode = \"RGBA\" if pm.alpha else \"RGB\"\n",
    "        img = Image.frombytes(mode, (pm.width, pm.height), pm.samples)\n",
    "        if pm.alpha:\n",
    "            img = img.convert(\"RGB\")\n",
    "        return ImageOps.autocontrast(img)\n",
    "\n",
    "    def _read_tables(self, pdf_path: str, page_no: int, try_stream: bool) -> List:\n",
    "        tables = []\n",
    "        try:\n",
    "            # lattice first (needs ruling lines; fewer false positives)\n",
    "            t_lat = camelot.read_pdf(pdf_path, pages=str(page_no), flavor=\"lattice\")\n",
    "            if t_lat and t_lat.n > 0:\n",
    "                tables = t_lat\n",
    "            elif try_stream:\n",
    "                t_str = camelot.read_pdf(pdf_path, pages=str(page_no), flavor=\"stream\")\n",
    "                if t_str and t_str.n > 0:\n",
    "                    tables = t_str\n",
    "        except Exception:\n",
    "            tables = []\n",
    "        return tables\n",
    "\n",
    "    def _table_is_valid(self, table) -> bool:\n",
    "        df = table.df\n",
    "        n_rows, n_cols = df.shape\n",
    "        if n_rows < self.min_table_rows or n_cols < self.min_table_cols:\n",
    "            return False\n",
    "        bad = 0\n",
    "        for r in range(n_rows):\n",
    "            cells = [str(v).strip() for v in list(df.iloc[r].values)]\n",
    "            if not _row_informative(cells):\n",
    "                bad += 1\n",
    "        return (bad / max(1, n_rows)) < 0.7\n",
    "\n",
    "    def _emit_table_docs(\n",
    "        self, table, base_meta: Dict[str, Any], table_index: int\n",
    "    ) -> List[Document]:\n",
    "        out: List[Document] = []\n",
    "        df = table.df\n",
    "        n_rows, n_cols = df.shape\n",
    "\n",
    "        # Optional: detect header row only if it looks like a header (few digits)\n",
    "        def looks_like_header(cells: List[str]) -> bool:\n",
    "            cell_txt = \" \".join(cells)\n",
    "            # header rows typically have fewer numbers\n",
    "            return NUMBER_RE.search(cell_txt) is None or len(cell_txt) < 40\n",
    "\n",
    "        headers = None\n",
    "        start_row = 0\n",
    "        if n_rows > 0:\n",
    "            first_row = [str(v).strip() for v in list(df.iloc[0].values)]\n",
    "            if looks_like_header(first_row):\n",
    "                headers = first_row\n",
    "                start_row = 1\n",
    "\n",
    "        # Whole-table doc (useful when rows are too many)\n",
    "        if not self.emit_table_rows or n_rows > self.max_rows_per_table:\n",
    "            table_text = \"\\n\".join(\n",
    "                [\"\\t\".join(str(v).strip() for v in list(df.iloc[r].values)) for r in range(start_row, n_rows)]\n",
    "            )\n",
    "            out.append(\n",
    "                Document(\n",
    "                    text=table_text[:8000],\n",
    "                    metadata={**base_meta, \"block_type\": \"table\", \"table_index\": table_index,\n",
    "                              \"n_rows\": int(n_rows), \"n_cols\": int(n_cols), \"headers\": headers},\n",
    "                )\n",
    "            )\n",
    "            return out\n",
    "\n",
    "        # Row-level docs with caps and filters\n",
    "        emitted = 0\n",
    "        for r in range(start_row, n_rows):\n",
    "            if emitted >= self.max_rows_per_table:\n",
    "                break\n",
    "            cells = [str(v).strip() for v in list(df.iloc[r].values)]\n",
    "            if not _row_informative(cells):\n",
    "                continue\n",
    "            row_text = \"\\t\".join(c for c in cells if c)\n",
    "            if not row_text:\n",
    "                continue\n",
    "            out.append(\n",
    "                Document(\n",
    "                    text=row_text,\n",
    "                    metadata={\n",
    "                        **base_meta,\n",
    "                        \"block_type\": \"table_row\",\n",
    "                        \"table_index\": table_index,\n",
    "                        \"row_index\": r,\n",
    "                        \"headers\": headers,\n",
    "                        \"n_cols\": int(n_cols),\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "            emitted += 1\n",
    "        return out\n",
    "\n",
    "    def load_data(self, file: str, extra_info: Optional[Dict[str, Any]] = None, **kwargs):\n",
    "        doc = fitz.open(file)\n",
    "        out_docs: List[Document] = []\n",
    "        total_nodes = 0\n",
    "\n",
    "        for i, page in enumerate(doc):\n",
    "            page_no = i + 1\n",
    "            page_meta = {\n",
    "                \"source\": os.path.abspath(file),\n",
    "                \"page\": page_no,\n",
    "                **(extra_info or {}),\n",
    "            }\n",
    "\n",
    "            # Base: page-level text or OCR\n",
    "            if self._has_meaningful_text(page):\n",
    "                text = (page.get_text(\"text\") or \"\").strip()\n",
    "                ocr_applied = False\n",
    "                page_doc = Document(text=text, metadata={**page_meta, \"ocr_applied\": ocr_applied, \"block_type\": \"page_text\"})\n",
    "                out_docs.append(page_doc)\n",
    "                total_nodes += 1\n",
    "            else:\n",
    "                img = self._page_to_image(page)\n",
    "                text = pytesseract.image_to_string(img, lang=self.lang, config=f\"--psm {self.ocr_psm}\").strip()\n",
    "                ocr_applied = True\n",
    "                page_doc = Document(text=text, metadata={**page_meta, \"ocr_applied\": ocr_applied, \"block_type\": \"ocr_page_text\"})\n",
    "                out_docs.append(page_doc)\n",
    "                total_nodes += 1\n",
    "                # Don’t try Camelot on OCR pages\n",
    "                continue\n",
    "\n",
    "            # Early stop if page is already too noisy\n",
    "            if total_nodes >= self.max_nodes_per_page * (i + 1):\n",
    "                continue\n",
    "\n",
    "            # Tables: only if enabled and page likely has rulings\n",
    "            if self.extract_tables:\n",
    "                ruled = _has_ruled_lines(page, min_lines=8)\n",
    "                tables = []\n",
    "                if ruled:\n",
    "                    tables = self._read_tables(file, page_no, try_stream=self.allow_stream)\n",
    "                # If not ruled and stream is allowed, still try stream but guarded\n",
    "                elif self.allow_stream:\n",
    "                    tables = self._read_tables(file, page_no, try_stream=True)\n",
    "\n",
    "                # Emit valid tables only\n",
    "                for t_idx, t in enumerate(tables or []):\n",
    "                    if not self._table_is_valid(t):\n",
    "                        continue\n",
    "                    t_docs = self._emit_table_docs(t, {**page_meta, \"ocr_applied\": ocr_applied}, t_idx)\n",
    "                    out_docs.extend(t_docs)\n",
    "                    total_nodes += len(t_docs)\n",
    "                    # Page-level cap\n",
    "                    if total_nodes >= self.max_nodes_per_page * (i + 1):\n",
    "                        break\n",
    "\n",
    "        return out_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af324544-3fcc-4589-b266-137857b4e090",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading test files\n",
    "reader = MixedPDFReader(dpi=300, lang=\"eng\", min_chars=40, ocr_psm=6)\n",
    "page_reader = SimpleDirectoryReader(\n",
    "    input_dir=\"test/\",\n",
    "    file_extractor={\".pdf\": reader}\n",
    ")\n",
    "documents = page_reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "266245ef-334e-4d21-ba88-3cdec2f98c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages Loaded: 59\n",
      "Loan Estimate\n",
      "Save this Loan Estimate to compare with your Closing Disclosure.\n",
      "DATE ISSUED\n",
      "APPLICANTS\n",
      "PROPERTY\n",
      "SALE PRICE\n",
      "LOAN TERM\n",
      "PURPOSE\n",
      "PRODUCT\n",
      "LOAN TYPE\n",
      "LOAN ID #\n",
      "RATE LOCK\n",
      "Conventional\n",
      "FHA\n",
      "VA\n",
      "NO\n",
      "YES, until\n",
      "Before closing, your interest rate, points, and lender credits can\n",
      "change unless you loc\n"
     ]
    }
   ],
   "source": [
    "print('Pages Loaded:',len(documents))\n",
    "print(documents[0].text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29d4b231-3b0c-46ba-8ea6-59cce0bff3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import LlamaGrammar\n",
    "\n",
    "def categories_gbnf():\n",
    "    alts = \" | \".join(f'\"{c}\"' for c in CATEGORIES)\n",
    "    return LlamaGrammar.from_string(f\"root ::= {alts}\\n\")\n",
    "\n",
    "YESNO_GBNF = \"root ::= \\\"yes\\\" | \\\"no\\\"\\n\"\n",
    "\n",
    "DECODE = dict(\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    "    top_k=30,\n",
    "    repeat_penalty=1.15,\n",
    ")\n",
    "\n",
    "CATEGORIES = [\n",
    "    \"Resume\", \"Contract\", \"LoanAgreement\", \"Invoice\", \"PaySlip\",\n",
    "    \"LenderFee\", \"LandDeed\", \"BankStatement\", \"TaxDocument\",\n",
    "    \"Insurance\", \"Report\", \"Letter\", \"Form\", \"ID\", \"Medical\", \"Other\"\n",
    "]\n",
    "\n",
    "def classify_document(text, max_chars=2000):\n",
    "    snippet = (text or \"\")[:max_chars]\n",
    "\n",
    "    system = (\n",
    "        \"You are a strict document classifier. \"\n",
    "        \"Return exactly one label from the allowed set. \"\n",
    "        \"No explanations. No extra words. No punctuation.\"\n",
    "    )\n",
    "    user = (\n",
    "        \"Allowed labels:\\n\"\n",
    "        + \", \".join(CATEGORIES) + \"\\n\"\n",
    "        +  \"\"\"Label Guide:\n",
    "        - Resume: CV, resume, and documents containing work history, usually one or two pages.\n",
    "        - Contract: general legal agreement not specific to mortgages, insurance, and property.\n",
    "        - LoanAgreement: loan agreement for home loan.\n",
    "        - Invoice: a bill requesting payment for goods/services.\n",
    "        - PaySlip: salary/wage statement for an employee.\n",
    "        - LenderFee: fee worksheet/closing cost breakdown, usually a single worksheet.\n",
    "        - LandDeed: Title document and property deed and documents for land ownership.\n",
    "        - BankStatement: transaction history of an account issued by a bank.\n",
    "        - TaxDocument: Tax return and tax form\n",
    "        - Insurance: Insurance policy documents\n",
    "        - Report: documents containing data analysis and findings\n",
    "        - Letter: correspondence communications\n",
    "        - Form: applications and other Form documents requiring user to enter data\n",
    "        - ID: documents used for identity checking and verficiation\n",
    "        - Medical: medical reports and health prescriptions\n",
    "        If unsure, choose Other\n",
    "        \"\"\"\n",
    "        + \"\"\"\\n\\nTask: Classify the following content into exactly ONE allowed label.\\n\\n\"\"\"\n",
    "        f\"Content:\\n{snippet}\\n\\n\"\n",
    "        \"Answer with ONLY the label.\"\n",
    "    )\n",
    "\n",
    "    out = llm.create_chat_completion(\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user},\n",
    "        ],\n",
    "        max_tokens = 4,\n",
    "        grammar = categories_gbnf(),\n",
    "        **DECODE\n",
    "    )\n",
    "    label = out[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "    # Optional guard: map minor variations back to canonical labels\n",
    "    return label\n",
    "\n",
    "def is_same_document(page1, page2, doc_type=None, max_chars=1200):\n",
    "    p1 = (page1 or \"\")[:max_chars]\n",
    "    p2 = (page2 or \"\")[:max_chars]\n",
    "    dtype = doc_type or \"Unknown\"\n",
    "\n",
    "    system = (\n",
    "        \"You decide if Page_2 starts a NEW document, given Page_1 and its document type. \"\n",
    "        \"Output exactly 'yes' or 'no'. No other text.\"\n",
    "    )\n",
    "    user = (\n",
    "        f\"Page_1 Document Type: {dtype}\\n\"\n",
    "        f\"Page_1:\\n{p1}\\n\\n\"\n",
    "        f\"Page_2:\\n{p2}\\n\\n\"\n",
    "        \"Question: Does Page_2 start a NEW document? Answer only 'yes' or 'no'.\"\n",
    "    )\n",
    "\n",
    "    out = llm.create_chat_completion(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user},\n",
    "        ],\n",
    "        max_tokens=2,\n",
    "        grammar = LlamaGrammar.from_string(YESNO_GBNF),\n",
    "        **DECODE\n",
    "    )\n",
    "    return out[\"choices\"][0][\"message\"][\"content\"].strip().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "460b9ebc-6376-4b3b-b0e5-93775a722742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '/mnt/d/ex_outo/test/appraisal_report.pdf',\n",
       " 'page': 2,\n",
       " 'file_path': '/mnt/d/ex_outo/test/appraisal_report.pdf',\n",
       " 'file_name': 'appraisal_report.pdf',\n",
       " 'file_type': 'application/pdf',\n",
       " 'file_size': 462164,\n",
       " 'creation_date': '2025-09-25',\n",
       " 'last_modified_date': '2025-09-25',\n",
       " 'ocr_applied': False,\n",
       " 'block_type': 'page_text'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[5].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dfaf43e-c9bf-4476-acdc-3d225b2c1a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Report'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing doctype function on test file\n",
    "classify_document(documents[5].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92a06cc9-ac4b-456c-a192-d308bbb5bb85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing document boundary function for test files\n",
    "is_same_document(documents[5].text, documents[6].text, doc_type = 'Report')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e296e2b6-2876-4ce8-ab6a-f630763986b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creating metadata and logical documents\n",
    "#this is most time consuming process, and better and faster LLM with better hardware can reduce the latency\n",
    "\n",
    "metadata = []\n",
    "current_doc_type = None\n",
    "page_in_doc = 0\n",
    "is_new_doc = True\n",
    "logical_documents = []\n",
    "\n",
    "for i, page in enumerate(documents):\n",
    "    if i == 0:\n",
    "        current_doc_type = classify_document(page.text)\n",
    "        text = page.text\n",
    "    else:\n",
    "        prev_text = documents[i - 1].text\n",
    "        output = is_same_document(prev_text, page.text, current_doc_type)\n",
    "        if output.startswith('y'):\n",
    "            current_doc_type = classify_document(page.text)\n",
    "            page_in_doc = 0\n",
    "            text = page.text\n",
    "            is_new_doc = True\n",
    "            \n",
    "        else:\n",
    "            page_in_doc += 1\n",
    "            text = text + \"\\n\\n\" + page.text\n",
    "            is_new_doc = False\n",
    "\n",
    "    metadata.append({\n",
    "        \"page\": i,\n",
    "        'is_new_doc': is_new_doc,\n",
    "        \"doc_type\": current_doc_type,\n",
    "        'page_in_doc': page_in_doc,\n",
    "        'source_file': page.metadata['file_name'],\n",
    "    })\n",
    "\n",
    "    if is_new_doc:\n",
    "        logical_documents.append({\n",
    "            'text': text,\n",
    "            'doc_type': current_doc_type,\n",
    "            'page_start': i,\n",
    "            'page_end': i\n",
    "        })\n",
    "    else:\n",
    "        logical_documents[-1]['page_end'] = i\n",
    "        logical_documents[-1]['text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c31073e-f32b-47a3-94dd-941e208f5fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page': 0,\n",
       "  'is_new_doc': True,\n",
       "  'doc_type': 'LoanAgreement',\n",
       "  'page_in_doc': 0,\n",
       "  'source_file': 'LoanEstimate.pdf'},\n",
       " {'page': 1,\n",
       "  'is_new_doc': False,\n",
       "  'doc_type': 'LoanAgreement',\n",
       "  'page_in_doc': 1,\n",
       "  'source_file': 'LoanEstimate.pdf'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28b27127-c393-4c8d-9fdf-b5c60718b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment it to save the metadata as it is base of logical documents and will take a lot of time to make it in local limited hardware\n",
    "import pickle\n",
    "with open('metadata_test.pkl', 'wb') as file:\n",
    "    pickle.dump(metadata, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "016d72b2-e5f8-4516-a711-8841425a50ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using a better LLM will help in decision boundaries and faster and even better processing of metadata creation\n",
    "len(logical_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "022ebbc3-f99d-428d-b0b3-6659383c0cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "2025-09-26 12:03:40,960 - INFO - Load pretrained SentenceTransformer: BAAI/bge-base-en-v1.5\n",
      "2025-09-26 12:03:44,984 - INFO - 1 prompt is loaded, with the key: query\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "#using a CPP wrapper based llama 3.1 for rag\n",
    "llm_rag = LlamaCPP(\n",
    "    model_path = 'llama_3.1_8b_q8.gguf',\n",
    "    temperature = 0.1,\n",
    "    context_window = 4096,\n",
    "    model_kwargs = {\"n_gpu_layers\": -1, 'n_batch': 256},\n",
    "    max_new_tokens = 64,\n",
    "    generate_kwargs={\"stop\": [\"\\n\", \"\\n\\n\", \"Reasoning\", \"Explanation:\", 'However']},\n",
    "    verbose = False\n",
    ")\n",
    "Settings.llm = llm_rag  \n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name = 'BAAI/bge-base-en-v1.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bb9e630e-5e28-43bc-a933-00efda5d2547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 1600, chunk_overlap = 100)\n",
    "\n",
    "chunked_documents = []\n",
    "\n",
    "for idx, doc in enumerate(logical_documents):\n",
    "    chunks = splitter.split_text(doc[\"text\"])\n",
    "    for chunk_idx, chunk in enumerate(chunks):\n",
    "        chunked_documents.append(\n",
    "            Document(\n",
    "                text=chunk,\n",
    "                metadata={\n",
    "                    \"doc_type\": doc[\"doc_type\"],\n",
    "                    \"chunk_index\": chunk_idx,\n",
    "                    \"page_start\": doc[\"page_start\"],\n",
    "                    \"page_end\": doc[\"page_end\"], \n",
    "                }\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "77e41c81-4c22-4276-bde5-3e2a60e0ddf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunked_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9d196cb0-03ca-4110-9e3e-ed9102bab065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#semantic splitting the logical documents chunked in recursive manner\n",
    "\n",
    "semantic_splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size = 15,                      # keeps neighboring sentences in context when deciding\n",
    "    breakpoint_percentile_threshold = 90,  # higher = fewer, stronger splits\n",
    "    embed_model = Settings.embed_model\n",
    ")\n",
    "\n",
    "# Convert coarse nodes back to Documents for the semantic splitter\n",
    "semantic_chunks = semantic_splitter.get_nodes_from_documents(chunked_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "706eeaba-2255-4719-b224-c05e3dd6a538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(semantic_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59e656f7-e278-4cce-9448-55d2ca14f604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 11:40:02,697 - INFO - Loading faiss with AVX512 support.\n",
      "2025-09-26 11:40:02,756 - INFO - Successfully loaded faiss with AVX512 support.\n"
     ]
    }
   ],
   "source": [
    "#creating a FAISS powered vector store\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "fa4c0185-36e4-40c0-99a4-9e576ee78ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c6f6b866-8b0c-429f-8e01-22d2ae66becf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "probe = Settings.embed_model.get_text_embedding(\"dimension probe\")\n",
    "dim = len(probe)\n",
    "\n",
    "faiss_index = faiss.IndexFlatL2(dim)\n",
    "vector_store = FaissVectorStore(faiss_index = faiss_index)  # FAISS as the backend\n",
    "\n",
    "#Build a VectorStoreIndex over semantic chunks\n",
    "storage_context = StorageContext.from_defaults(vector_store = vector_store)\n",
    "index = VectorStoreIndex(semantic_chunks, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "173b3de5-94ab-45d1-bab0-49cbf51a9507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will be used for metadata filtering for queries\n",
    "def predict_doc_type_for_query(query):\n",
    "    \"\"\"\n",
    "    Return a predicted query type so that appropriate chunks could be recalled \n",
    "    \"\"\"\n",
    "    system = (\n",
    "    f\"\"\"\n",
    "    You are an intelligent assistant that routes user queries to the most relevant document.\n",
    "    Choose ONLY ONE from:  {\", \".join(CATEGORIES)}\n",
    "    \n",
    "    \"\"\"\n",
    "    )\n",
    "    user = (f'Which document type most likely contain answer for my query: {query}. Give only one document type as answer.')\n",
    "\n",
    "    out = llm.create_chat_completion(\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user},\n",
    "        ],\n",
    "        max_tokens = 4,\n",
    "        grammar = categories_gbnf(),\n",
    "        **DECODE\n",
    "    )\n",
    "    label = out[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "9e565192-76dd-4e20-a2c9-796fed625c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.postprocessor.types import BaseNodePostprocessor\n",
    "\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "\n",
    "QA_PROMPT = PromptTemplate(\n",
    "    \"Use only the provided context. If the answer is not present, output exactly: Not found\\n\"\n",
    "    \"Return only the answer text, one short line. Do not add explanations, notes, or extra words.\\n\\n\"\n",
    "    \"Context:\\n{context_str}\\n\\n\"\n",
    "    \"Question: {query_str}\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "\n",
    "resp_synth = get_response_synthesizer(\n",
    "    response_mode=\"compact\",\n",
    "    text_qa_template=QA_PROMPT,\n",
    ")\n",
    "\n",
    "FIELD_BOOSTS = {\n",
    "    \"loan amount\": [\"loan amount\", \"amount financed\", \"base loan\", \"principal\"],\n",
    "    \"interest rate\": [\"interest rate\", \"annual percentage rate\", \"apr\", \"rate\"],\n",
    "    \"down payment\": [\"down payment\", \"cash to close\", \"funds due from borrower\"],\n",
    "    \"property_address\": [\"property address\", \"subject property\", \"address\"],\n",
    "    \"applicants\": [\"applicant\", \"borrower\", \"co-borrower\", \"name\"]\n",
    "}\n",
    "\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import QueryBundle\n",
    "\n",
    "\n",
    "MONEY_Q = re.compile(r'\\b(loan\\s*amount|amount\\s*financed|base\\s*loan|principal|origination|funding)\\b', re.I)\n",
    "GEN_MONEY_Q = re.compile(r'\\b(amount|loan|principal|balance|financed)\\b', re.I)\n",
    "\n",
    "def expand_query_for_bm25(query: str) -> str:\n",
    "    q = (query or \"\").strip()\n",
    "    if not q:\n",
    "        return q\n",
    "\n",
    "    boosts = []\n",
    "    # Strong rule: exact field intents\n",
    "    if MONEY_Q.search(q):\n",
    "        boosts += [\n",
    "            \"loan amount\",\n",
    "            \"amount financed\",\n",
    "            \"base loan\",\n",
    "            \"principal\",\n",
    "        ]\n",
    "    # Weak rule: generic money wording; add softer anchors\n",
    "    elif GEN_MONEY_Q.search(q):\n",
    "        boosts += [\n",
    "            \"loan amount\",\n",
    "            \"amount financed\",\n",
    "        ]\n",
    "\n",
    "    if not boosts:\n",
    "        return q\n",
    "\n",
    "    # BM25 is bag-of-words; adding tokens increases matches (no need for boolean OR)\n",
    "    return f\"{q} \" + \" \".join(boosts)\n",
    "\n",
    "\n",
    "class ExpandedBM25Retriever(BaseRetriever):\n",
    "    def __init__(self, base_bm25_retriever, expander=expand_query_for_bm25):\n",
    "        super().__init__()\n",
    "        self.base = base_bm25_retriever\n",
    "        self.expander = expander\n",
    "\n",
    "    def _retrieve(self, query_bundle, **kwargs):\n",
    "        q = query_bundle.query_str or \"\"\n",
    "        expanded = self.expander(q)\n",
    "        return self.base.retrieve(QueryBundle(query_str=expanded))\n",
    "\n",
    "# Pydantic-friendly client-side metadata filter with graceful fallback\n",
    "class DocTypeFilterPostprocessor(BaseNodePostprocessor):\n",
    "    doc_type: Optional[str] = None\n",
    "\n",
    "    # implement required abstract method for your LlamaIndex version\n",
    "    def _postprocess_nodes(self, nodes, query_bundle=None):\n",
    "        if not self.doc_type:\n",
    "            return nodes\n",
    "        filtered = [n for n in nodes if (n.node.metadata or {}).get(\"doc_type\") == self.doc_type]\n",
    "        return filtered if filtered else nodes  # fallback to unfiltered if empty\n",
    "\n",
    "\n",
    "def build_rag_pipeline(index, llm, query = None, doc_type_filtering = False,\n",
    "                       k_per_retriever = 8, final_top_n = 3, num_queries = 3):\n",
    "    all_nodes = list(index.docstore.docs.values())\n",
    "\n",
    "    # Doc-type routing (soft, client-side)\n",
    "    predicted_doc_type = None\n",
    "    if doc_type_filtering and query:\n",
    "        label = predict_doc_type_for_query(query)\n",
    "        if label and label.strip().lower() != \"other\":\n",
    "            predicted_doc_type = label.strip()\n",
    "\n",
    "    k_per = max(2, min(int(k_per_retriever), len(all_nodes)))\n",
    "    final_k = max(1, min(int(final_top_n), k_per))\n",
    "\n",
    "    # Vector retriever (no server-side filters; FAISS-safe)\n",
    "    vec = index.as_retriever(similarity_top_k=k_per)\n",
    "\n",
    "    # BM25 over all (or bias to doc_type by prefiltering node list)\n",
    "    bm25_nodes = (\n",
    "        [n for n in all_nodes if n.metadata.get(\"doc_type\") == predicted_doc_type] or all_nodes\n",
    "    ) if predicted_doc_type else all_nodes\n",
    "\n",
    "    bm25_base = BM25Retriever.from_defaults(nodes=bm25_nodes, similarity_top_k=k_per)\n",
    "\n",
    "    # Wrap with query expander\n",
    "    bm25_boosted = ExpandedBM25Retriever(bm25_base, expander=expand_query_for_bm25)\n",
    "\n",
    "    # Fuse vector + boosted BM25; QueryFusionRetriever will RRF over both\n",
    "    fusion = QueryFusionRetriever(\n",
    "        retrievers=[vec, bm25_boosted],\n",
    "        llm=llm,\n",
    "        similarity_top_k=k_per,\n",
    "        num_queries=num_queries,   \n",
    "        mode=\"reciprocal_rerank\",\n",
    "    )\n",
    "\n",
    "    # Postprocess: soft doc_type filter then cross-encoder rerank\n",
    "    doc_filter = DocTypeFilterPostprocessor(doc_type=predicted_doc_type)\n",
    "    reranker = SentenceTransformerRerank(\n",
    "        model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\",\n",
    "        top_n=final_k\n",
    "    )\n",
    "\n",
    "    qe = RetrieverQueryEngine.from_args(\n",
    "        retriever=fusion,\n",
    "        llm=llm,\n",
    "        response_synthesizer=resp_synth,\n",
    "        node_postprocessors=[doc_filter, reranker],\n",
    "        verbose=False\n",
    "    )\n",
    "    return qe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "4d82fe6c-cefa-4d68-936a-ad9e658bd5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "PLACEHOLDER_LINE = re.compile(r'^\\s*[_\\-\\.\\$€¥\\s]{3,}\\s*$', re.I)\n",
    "NOISY_FOOTERS = [\n",
    "    \"Search results\", \"Source [\", \"Explanation:\", \"Note:\"\n",
    "]\n",
    "\n",
    "def finalize_answer_minimal(raw_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Keep exactly one, clean line:\n",
    "    - Take the substring before the first 'Not found' (case-insensitive).\n",
    "    - Remove obvious footers/parentheticals.\n",
    "    - Return the first meaningful sentence/line; otherwise 'Not found'.\n",
    "    \"\"\"\n",
    "    s = (raw_text or \"\").strip()\n",
    "    if not s:\n",
    "        return \"Not found\"\n",
    "\n",
    "    # Cut everything from the first 'Not found' onwards\n",
    "    m = re.search(r'\\bnot\\s*found\\b', s, flags=re.I)\n",
    "    if m:\n",
    "        s = s[:m.start()].strip()\n",
    "\n",
    "    # Remove parentheticals like \"(Note: ...)\"\n",
    "    s = re.sub(r'\\([^)]*\\)', '', s).strip()\n",
    "\n",
    "    # Cut noisy footers if present\n",
    "    for marker in NOISY_FOOTERS:\n",
    "        if marker in s:\n",
    "            s = s.split(marker, 1)[0].strip()\n",
    "\n",
    "    # First sentence or first non-empty line\n",
    "    parts = re.split(r'(?<=[.!?])\\s+|\\n+', s)\n",
    "    first = next((p.strip() for p in parts if p and p.strip()), \"\")\n",
    "\n",
    "    # Guard against placeholders or empty fragments\n",
    "    if not first or PLACEHOLDER_LINE.match(first):\n",
    "        return \"Not found\"\n",
    "\n",
    "    # De-duplicate blunt repeats: keep up to the first clause\n",
    "    first = re.split(r'[.;]\\s*', first)[0].strip()\n",
    "\n",
    "    return first[:200] if first else \"Not found\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "8ed8fa5f-0ee4-40d5-822a-634457bd8c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 13:33:28,698 - DEBUG - Building index from IDs objects\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c61f55dd35545e393de997d01693d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/799 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c2ffa050584cd4bed1724467c7488e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3ed2cc01564e62bedb42a7b56d91b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e1f81a7984488fb2ebc298a036423c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e44a3234dd4240999d141dac90b6b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e89301f89cc4099880a83d95d267303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc29c6d2bdcf4dd995a2f220c95e6ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#testing rag initialization\n",
    "rag_engine_base = build_rag_pipeline(index, llm_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9c3aa6e2-44a3-42a2-ae94-5783e7e4c914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "c842b0d1-b4e2-4315-b3b7-4c1e58cab5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 18:46:11,256 - DEBUG - Building index from IDs objects\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769e491369644aac9208a657f72dc227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Response:\n",
      " ---------------------- \n",
      "\n",
      "A residential neighborhood comprised predominantly of 2-3 story, wood frame, row style, and detached SFRs\n",
      "\n",
      "Query execution time: 46.602 seconds\n"
     ]
    }
   ],
   "source": [
    "#test query\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "query = \"Summarize Neighborhood Description in the appraisal report.\"\n",
    "rag_engine = build_rag_pipeline(index, llm_rag)\n",
    "response = rag_engine.query(query)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "raw = str(response)\n",
    "final = finalize_answer_minimal(raw)\n",
    "\n",
    "print('\\nFinal Response:\\n ---------------------- \\n')\n",
    "print(final)  # print the cleaned, single-line answer\n",
    "print(f\"\\nQuery execution time: {elapsed_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "9750ddd5-f58b-4618-a4a5-291e659cbff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeWithScore(node=TextNode(id_='3a2394b7-01f8-4776-ba9c-574e77f917d2', embedding=None, metadata={'doc_type': 'Report', 'chunk_index': 8, 'page_start': 3, 'page_end': 6}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='9175d30f-95d5-47b0-a79c-e443657e7f5d', node_type='4', metadata={'doc_type': 'Report', 'chunk_index': 8, 'page_start': 3, 'page_end': 6}, hash='31562245cdfb091889959d6efe7b3e821f83cac27e1eb4acd2e882b9aff48d90'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='48372990-af0b-4fa4-8139-28c8edd59f0c', node_type='1', metadata={}, hash='323d00d998670a8d302334a1e6f7d30747fb4a748ead54a96ffb283a7e38c894')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='ClickFORMS Appraisal Software 800-622-8727\\nNet Adj: -5%\\nGross Adj : 6%\\nNet Adj: 16%\\nGross Adj: 16%\\nNet Adj: -1%\\nGross Adj: 8%\\nFile No.\\nUniform Residential Appraisal Report\\nThere are\\ncomparable properties currently offered for sale in the subject neighborhood ranging in price from $\\nto $\\n.\\nThere are\\ncomparable sales in the subject neighborhood within the past twelve months ranging in sale price from $\\nto $\\n.\\nFEATURE\\nSUBJECT\\nCOMPARABLE SALE # 1\\nCOMPARABLE SALE # 2\\nCOMPARABLE SALE # 3\\nAddress\\nProximity to Subject\\nSale Price\\n$\\n$\\n$\\n$\\nSale Price/Gross Liv. Area\\n$\\nsq. ft. $\\nsq. ft.\\n$\\nsq. ft.\\n$\\nsq. ft.\\nData Source(s)\\nVerification Source(s)\\nVALUE ADJUSTMENTS\\nDESCRIPTION\\nDESCRIPTION\\n+(-) $ Adjustment\\nDESCRIPTION\\n+(-) $ Adjustment\\nDESCRIPTION\\n+(-) $ Adjustment\\nSale or Financing\\nConcessions\\nDate of Sale/Time\\nLocation\\nLeasehold/Fee Simple\\nSite\\nView\\nDesign (Style)\\nQuality of Construction\\nActual Age\\nCondition\\nAbove Grade\\nTotal\\nBdrms. Baths\\nTotal Bdrms. Baths\\nTotal Bdrms. Baths\\nTotal Bdrms. ', mimetype='text/plain', start_char_idx=0, end_char_idx=990, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=np.float32(-1.8679119))"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-27 00:36:26,577 - DEBUG - Building index from IDs objects\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7cf67b62f334b8c910cd4c663f70718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-27 02:13:31,864 - DEBUG - Building index from IDs objects\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf6125d437148d8a0c4545470646012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44cb6ae46e24f20be8dd03470cc9485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae3d2ad83b244fc8569c89c0fa3cad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b53662693449348f849a786dd00c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c94728a59d493c86abb011b1ff36a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d7a9d6547e434bbaf6ab02b89a9f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bea13584c1746c38b7ad7d67e04a8a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b6d6ddc23e4e809de7f4e9024a8be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-27 02:58:21,034 - DEBUG - Building index from IDs objects\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a15c217c51674f3e9a460a19d8d027fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0a55c3b5aa4cd59450c765bb814fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-27 03:06:13,650 - DEBUG - Building index from IDs objects\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70574b8ce27a4c9082492bfbaa5ebc75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response.source_nodes[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1083bdbd-0997-42c5-86aa-bab101c9b11c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "31f52f2f-a889-4918-94f7-1a2ed3921d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 14:29:00,454 - DEBUG - Building index from IDs objects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MRR': 0.3679, 'Recall@1': 0.1, 'HitRate@1': 0.1333, 'Recall@3': 0.5, 'HitRate@3': 0.5333, 'Recall@5': 0.7333, 'HitRate@5': 0.7333, 'Recall@8': 0.8667, 'HitRate@8': 0.8667, 'Recall@10': 0.8667, 'HitRate@10': 0.8667}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# 1) Build gold_ids_by_query (yours is fine)\n",
    "queries = [\n",
    "    \"What is the name of person in the Driver's License ID?\",\n",
    "    \"What is the gross adjusted sale price of Comparable Sale Number 1 in the appraisal report?\",\n",
    "    \"What is the purpose of Appraisal Report?\",\n",
    "    \"What is the address for the lot in parcel number 1?\",\n",
    "    \"What is the total Loan estimate?\",\n",
    "    \"What is the property address for which loan is being taken?\",\n",
    "    \"What are the names of applicants in loan estimate sheet?\",\n",
    "    \"What is the closing balance in the account statement?\",\n",
    "    \"What are the services you cant shop for? Mention them and give their total\",\n",
    "    \"What is the name of employee in payslip and what is the Net Pay?\",\n",
    "    \"When was the last deposit made as per the the Bank Statement\",\n",
    "    \"What are the Total Closing Cost as per Loan Estimate?\",\n",
    "    \"Sumamrize Neighborhood Description in the appraisal report\",\n",
    "    \"Give breakdown of the total livable area floor wise for the property concerned in appraisal report.\",\n",
    "    \"What are the wages for Tina in W-2 form\",\n",
    "]\n",
    "\n",
    "node_ids_in_order = [\n",
    "    \"c72acb7b-4666-4f18-a363-3f8812c7cc5f\",\n",
    "    \"1b64170b-2970-459d-b831-2fed7d30acf7\",\n",
    "    \"2f44a985-ba5e-431d-baf6-7ca72ed61d26\",\n",
    "    \"64f89ec0-9d23-4784-af24-e28201ca692d\",\n",
    "    \"f47a18ec-65d6-4685-9a89-66810594c4ac\",\n",
    "    \"00bb7870-6f91-46bb-b952-2fe8930d714d\",\n",
    "    \"00bb7870-6f91-46bb-b952-2fe8930d714d\",\n",
    "    \"c17473d5-96d7-45f5-9ac7-4475c1cf209b\",\n",
    "    \"180bffca-5f4a-4bb1-a428-e52dfa896ff0\",\n",
    "    \"c72acb7b-4666-4f18-a363-3f8812c7cc5f\",\n",
    "    \"c17473d5-96d7-45f5-9ac7-4475c1cf209b\",\n",
    "    \"180bffca-5f4a-4bb1-a428-e52dfa896ff0\",\n",
    "    \"e0633fdf-82ac-4f3a-85d3-29791a3a3567\",\n",
    "    \"ca60a252-5a24-485c-9f91-245f365cdbf4\",\n",
    "    \"248969cd-9932-4972-ba3a-248172195cbb\",\n",
    "]\n",
    "\n",
    "assert len(queries) == len(node_ids_in_order), \"Queries and node_id list must have same length\"\n",
    "\n",
    "gold_ids_by_query = {q: {node_ids_in_order[i]} for i, q in enumerate(queries)}\n",
    "gold_ids_by_query[queries[13]].add(\"3821d3b2-2702-4959-a2f4-0dc34c445c44\")  # extra gold for 14th query\n",
    "\n",
    "# Optional: verify gold ids exist in index (won’t crash if index structure differs)\n",
    "try:\n",
    "    all_node_ids = set(getattr(index.docstore, \"docs\", {}).keys())\n",
    "    missing = [nid for nid in set().union(*gold_ids_by_query.values()) if nid not in all_node_ids]\n",
    "    if missing:\n",
    "        print(\"Warning: some gold node_ids not present in the index:\", missing)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 2) Robust node_id extraction (handles NodeWithScore variants)\n",
    "def _node_id_from_hit(hit):\n",
    "    nid = getattr(hit, \"node_id\", None)\n",
    "    if nid:\n",
    "        return nid\n",
    "    node = getattr(hit, \"node\", None)\n",
    "    if node is not None:\n",
    "        return getattr(node, \"node_id\", None)\n",
    "    return None\n",
    "\n",
    "# 3) Evaluator: pass retriever in\n",
    "def eval_retrieval(retriever, eval_queries, gold_ids_by_query, Ks=(1,3,5,10)):\n",
    "    Ks = sorted(set(Ks))\n",
    "    maxK = max(Ks)\n",
    "    # Ensure your retriever is configured to return at least maxK hits.\n",
    "    # If not, rebuild it or set similarity_top_k accordingly before calling this.\n",
    "\n",
    "    recall_at = defaultdict(float)\n",
    "    hit_at = defaultdict(float)\n",
    "    mrr_sum = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for q in eval_queries:\n",
    "        gold = set(gold_ids_by_query.get(q, []))\n",
    "        if not gold:\n",
    "            continue\n",
    "        n += 1\n",
    "\n",
    "        hits = retriever.retrieve(q) or []\n",
    "        ranked_ids = []\n",
    "        for h in hits:\n",
    "            nid = _node_id_from_hit(h)\n",
    "            if nid:\n",
    "                ranked_ids.append(nid)\n",
    "\n",
    "        # MRR\n",
    "        rr = 0.0\n",
    "        for rank, nid in enumerate(ranked_ids, start=1):\n",
    "            if nid in gold:\n",
    "                rr = 1.0 / rank\n",
    "                break\n",
    "        mrr_sum += rr\n",
    "\n",
    "        # Recall@K + HitRate@K\n",
    "        for K in Ks:\n",
    "            topK = ranked_ids[:K]\n",
    "            retrieved_rel = sum(1 for nid in topK if nid in gold)\n",
    "            recall_at[K] += retrieved_rel / max(1, len(gold))\n",
    "            hit_at[K]    += 1.0 if retrieved_rel > 0 else 0.0\n",
    "\n",
    "    if n == 0:\n",
    "        return {\"note\": \"no labeled queries\"}\n",
    "\n",
    "    out = {\"MRR\": round(mrr_sum / n, 4)}\n",
    "    for K in Ks:\n",
    "        out[f\"Recall@{K}\"] = round(recall_at[K] / n, 4)\n",
    "        out[f\"HitRate@{K}\"] = round(hit_at[K] / n, 4)\n",
    "    return out\n",
    "\n",
    "# 4) Build once, reuse retriever (ensure top_k >= 10 if you want Recall@10 to be meaningful)\n",
    "qe = build_rag_pipeline(index, llm_rag, k_per_retriever=10, final_top_n=10)\n",
    "retriever = qe._retriever\n",
    "\n",
    "metrics = eval_retrieval(retriever, queries, gold_ids_by_query, Ks=(1,3,5,8,10))\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f0657585-3533-4001-85ea-262c26c689d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 13:57:13,402 - DEBUG - Building index from IDs objects\n",
      "2025-09-26 13:57:20,000 - DEBUG - Building index from IDs objects\n",
      "2025-09-26 13:57:27,996 - DEBUG - Building index from IDs objects\n",
      "2025-09-26 13:57:35,064 - DEBUG - Building index from IDs objects\n",
      "2025-09-26 13:57:35,066 - WARNING - As bm25s.BM25 requires k less than or equal to number of nodes added. Overriding the value of similarity_top_k to number of nodes added.\n",
      "2025-09-26 13:57:41,089 - DEBUG - Building index from IDs objects\n",
      "2025-09-26 13:57:47,509 - DEBUG - Building index from IDs objects\n",
      "2025-09-26 13:57:53,529 - DEBUG - Building index from IDs objects\n",
      "2025-09-26 13:57:59,616 - DEBUG - Building index from IDs objects\n",
      "2025-09-26 13:58:05,975 - DEBUG - Building index from IDs objects\n",
      "2025-09-26 13:58:12,772 - DEBUG - Building index from IDs objects\n",
      "2025-09-26 13:58:19,283 - DEBUG - Building index from IDs objects\n",
      "2025-09-26 13:58:25,541 - DEBUG - Building index from IDs objects\n",
      "2025-09-26 13:58:32,309 - DEBUG - Building index from IDs objects\n",
      "2025-09-26 13:58:39,009 - DEBUG - Building index from IDs objects\n",
      "2025-09-26 13:58:45,822 - DEBUG - Building index from IDs objects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MRR': 0.2978, 'Recall@1': 0.0667, 'HitRate@1': 0.0667, 'Recall@3': 0.4667, 'HitRate@3': 0.4667, 'Recall@5': 0.6667, 'HitRate@5': 0.6667, 'Recall@8': 0.6667, 'HitRate@8': 0.6667, 'Recall@10': 0.7333, 'HitRate@10': 0.7333}\n"
     ]
    }
   ],
   "source": [
    "def eval_retrieval_filter(eval_queries, gold_ids_by_query, Ks=(1,3,5,10)):   \n",
    "    \n",
    "    Ks = sorted(set(Ks))\n",
    "    maxK = max(Ks)\n",
    "    # Ensure your retriever is configured to return at least maxK hits.\n",
    "    # If not, rebuild it or set similarity_top_k accordingly before calling this.\n",
    "\n",
    "    recall_at = defaultdict(float)\n",
    "    hit_at = defaultdict(float)\n",
    "    mrr_sum = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for q in eval_queries:\n",
    "        qe = build_rag_pipeline(index, llm_rag, query = q, doc_type_filtering= True, k_per_retriever = 10)\n",
    "        retriever = qe._retriever\n",
    "        gold = set(gold_ids_by_query.get(q, []))\n",
    "        if not gold:\n",
    "            continue\n",
    "        n += 1\n",
    "\n",
    "        hits = retriever.retrieve(q) or []\n",
    "        ranked_ids = []\n",
    "        for h in hits:\n",
    "            nid = _node_id_from_hit(h)\n",
    "            if nid:\n",
    "                ranked_ids.append(nid)\n",
    "\n",
    "        # MRR\n",
    "        rr = 0.0\n",
    "        for rank, nid in enumerate(ranked_ids, start=1):\n",
    "            if nid in gold:\n",
    "                rr = 1.0 / rank\n",
    "                break\n",
    "        mrr_sum += rr\n",
    "\n",
    "        # Recall@K + HitRate@K\n",
    "        for K in Ks:\n",
    "            topK = ranked_ids[:K]\n",
    "            retrieved_rel = sum(1 for nid in topK if nid in gold)\n",
    "            recall_at[K] += retrieved_rel / max(1, len(gold))\n",
    "            hit_at[K]    += 1.0 if retrieved_rel > 0 else 0.0\n",
    "\n",
    "    if n == 0:\n",
    "        return {\"note\": \"no labeled queries\"}\n",
    "\n",
    "    out = {\"MRR\": round(mrr_sum / n, 4)}\n",
    "    for K in Ks:\n",
    "        out[f\"Recall@{K}\"] = round(recall_at[K] / n, 4)\n",
    "        out[f\"HitRate@{K}\"] = round(hit_at[K] / n, 4)\n",
    "    return out\n",
    "\n",
    "metrics = eval_retrieval_filter(queries, gold_ids_by_query, Ks=(1,3,5,8,10))\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0502dd1-03c0-4d92-927b-cf728c52b595",
   "metadata": {},
   "source": [
    "Even if we added soft metadata routing only and it worked fine in training set, in test files it is giving worse result than default routing. This happens because our Llama 3.1 8B itself is limited and is not able to decide on the document type properly. We got near perfect result with Gemini 2.5 flash and other similar commercial grade LLMs, but they are not as secure and cheap compared to our local LLM, we can run a better offline LLM like Deepseek R1 and Grok 2 on much more capable machine but for our project we will not be getting much improvements until LLM itself is not able to route metadata properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e93683dc-14dd-40be-9629-2eb9176200b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def process_queries(queries):\n",
    "    rag_engine = build_rag_pipeline(index, llm_rag, final_top_n = 3, k_per_retriever = 8) \n",
    "    time_list = []\n",
    "    for i in range(len(queries)):\n",
    "        print(f\"Query No. {i+1}:\\n\"\n",
    "        f\"{queries[i]}\\n\"\n",
    "        \"----------------------\\n\"\n",
    "        )\n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = rag_engine.query(queries[i])\n",
    "        raw  = str(response)\n",
    "        final = finalize_answer_minimal(raw)\n",
    "        \n",
    "        end_time = time.time()  \n",
    "        elapsed_time = end_time - start_time  # Calculate elapsed time in seconds\n",
    "        time_list.append(elapsed_time)\n",
    "        print('\\nFinal Response:\\n ---------------------- \\n')\n",
    "        print(final)\n",
    "        print(\"==============================\\n\")\n",
    "        \n",
    "    print(f\"\"\"\\n Average time taken for processing queries: {np.mean(time_list):.3f} seconds.\\n\n",
    "    Maximum time taken for processing a single query: {np.max(time_list):.3f} seconds.\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "abb788bd-003f-4b57-81ee-e1451d2065c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 14:12:21,139 - DEBUG - Building index from IDs objects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query No. 1:\n",
      "What is the name of person in the Driver's License ID?\n",
      "----------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3c5de3a857408d84cc002229ca6602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Response:\n",
      " ---------------------- \n",
      "\n",
      "123 -567 sss avin | Reece 68 eye wer 1401br> O111/66 fh Conon miO(/o\n",
      "==============================\n",
      "\n",
      "Query No. 2:\n",
      "What is the gross adjusted sale price of Comparable Sale Number 1 in the appraisal report?\n",
      "----------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc11183954ba455da0a620210c95663b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Response:\n",
      " ---------------------- \n",
      "\n",
      "$1,073,000\n",
      "==============================\n",
      "\n",
      "Query No. 3:\n",
      "What is the purpose of Appraisal Report?\n",
      "----------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c438e23da6043a4bb7e971cb6cb5d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Response:\n",
      " ---------------------- \n",
      "\n",
      "To provide the lender/client with an accurate, and adequately supported, opinion of the market value of the subject property\n",
      "==============================\n",
      "\n",
      "Query No. 4:\n",
      "What is the address for the lot in parcel number 1?\n",
      "----------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02270a5b196748b683c0058a3b04eb8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Response:\n",
      " ---------------------- \n",
      "\n",
      "58 of Villa Barbados North Unit No\n",
      "==============================\n",
      "\n",
      "Query No. 5:\n",
      "What is the total Loan estimate?\n",
      "----------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a708331e374483e8ae8f5b2dd483b15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Response:\n",
      " ---------------------- \n",
      "\n",
      "$117,339\n",
      "==============================\n",
      "\n",
      "Query No. 6:\n",
      "What is the property address for which loan is being taken?\n",
      "----------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae68e2cc38a942c2bae9ab364847be34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Response:\n",
      " ---------------------- \n",
      "\n",
      "Not found\n",
      "==============================\n",
      "\n",
      "Query No. 7:\n",
      "What are the names of applicants in loan estimate sheet?\n",
      "----------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126af152a0b94705bd40205d294c3a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Response:\n",
      " ---------------------- \n",
      "\n",
      "John Q\n",
      "==============================\n",
      "\n",
      "Query No. 8:\n",
      "What is the closing balance in the account statement?\n",
      "----------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10facd0b5055475992cae83076accb8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Response:\n",
      " ---------------------- \n",
      "\n",
      "51,176\n",
      "==============================\n",
      "\n",
      "Query No. 9:\n",
      "What are the services you cant shop for? Mention them and give their total\n",
      "----------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8416e2002105460a8389d30ea551583d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Response:\n",
      " ---------------------- \n",
      "\n",
      "B\n",
      "==============================\n",
      "\n",
      "Query No. 10:\n",
      "What is the name of employee in payslip and what is the Net Pay?\n",
      "----------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f595be5cc345d59bf263fac52471a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Response:\n",
      " ---------------------- \n",
      "\n",
      "Sally Harley, 9500\n",
      "==============================\n",
      "\n",
      "Query No. 11:\n",
      "When was the last deposit made as per the the Bank Statement\n",
      "----------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160308d7c4b34f619838d4da91ef9dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Response:\n",
      " ---------------------- \n",
      "\n",
      "07/31/2018\n",
      "==============================\n",
      "\n",
      "Query No. 12:\n",
      "What are the Total Closing Cost as per Loan Estimate?\n",
      "----------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c8f9049da14183ae290612e3356d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Response:\n",
      " ---------------------- \n",
      "\n",
      "$5,802\n",
      "==============================\n",
      "\n",
      "Query No. 13:\n",
      "Sumamrize Neighborhood Description in the appraisal report\n",
      "----------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158470a4f28c4d559ed49ea0c4381a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Response:\n",
      " ---------------------- \n",
      "\n",
      "A residential neighborhood comprised predominantly of 2-3 story, wood frame, row style, and detached SFRs\n",
      "==============================\n",
      "\n",
      "Query No. 14:\n",
      "Give breakdown of the total livable area floor wise for the property concerned in appraisal report.\n",
      "----------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26cfc5fc19244a784df0d78eeff3e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Response:\n",
      " ---------------------- \n",
      "\n",
      "First Floor: 948\n",
      "==============================\n",
      "\n",
      "Query No. 15:\n",
      "What are the wages for Tina in W-2 form\n",
      "----------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a756eb06dc4c46f1908e967b0570bf33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Response:\n",
      " ---------------------- \n",
      "\n",
      "8489\n",
      "==============================\n",
      "\n",
      "\n",
      " Average time taken for processing queries: 42.492 seconds.\n",
      "\n",
      "    Maximum time taken for processing a single query: 57.680 seconds.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "process_queries(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "85390ff7-2f92-4767-b4f5-110b5cbe4fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(response.source_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2574b338-43ba-4cda-a4fc-61ebeaa6bcfd",
   "metadata": {},
   "source": [
    "We got 66.67% accuracy with 10 queries having valid answers. However, relative accuracy of some of them have issues, for example Wages of Tina are 8489.27, due to strict answer formatting we were made, still relative accuracy is at 83%.\n",
    "\n",
    "Overall, both accuracy, relative numerical accuracy, and latency could be improved a lot by using a better LLM. And also, retriever could be improved by an actual working metadata filtering and query routing which every Local Instruct LLM we tested failed to do as we intended. We could swap a better LLM here or use count based method where we determine the doctype on the basis of most common words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab87d16-cce4-4151-9db3-65858650a1d8",
   "metadata": {},
   "source": [
    "## Gradio APP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eca87b-287b-478c-8188-27b17bd644f5",
   "metadata": {},
   "source": [
    "In this part of the notebook we will wrap the essential codes for the Gradio application which would allow users to access the RAG Pipeline running on local machine. Most of the code would be repeated and used for gradio specifically, we can create a different notebook for this but since this is one project we will combine them both. nevertheless code of section below could be used seperately for the app.\n",
    "\n",
    "App highlights:\n",
    "1. Upload PDFs → Build/refresh index with a visible spinner overlay and progress updates.\n",
    "2. Ask a question in a large input box; Send/Clear are compact and embedded beside it.\n",
    "3. Immediate chat experience: user message appears first; a temporary “Working…” bubble is shown; then the final answer replaces it.\n",
    "4. Top 3 Chunks table with score and source details.\n",
    "5. PDF Viewer to see uploaded files.\n",
    "6. One simple control that matters: the Retrieve top‑k slider, which balances recall vs. speed.\n",
    "7. Strict QA prompt + short generation cap to keep answers crisp.\n",
    "8. Output finalizer to avoid repeated text (numbers or sentences).\n",
    "\n",
    "And while most of the pipeline is same we had to remove Logical Documents chunking as even index of 3-4 documents was taking 5-8 minutes to build the index when without it, we needed only few seconds to make the index. If we could improve the quality and latency Document Segmentation and Query Routing we could add them back for gradio app, but most immediate method to improve them is use API based LLM and for this project we are restricted from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7619e3f5-dafd-4287-bb4e-b39b79adb3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "2025-09-27 15:50:12,811 - INFO - Load pretrained SentenceTransformer: BAAI/bge-base-en-v1.5\n",
      "2025-09-27 15:50:17,478 - INFO - 1 prompt is loaded, with the key: query\n"
     ]
    }
   ],
   "source": [
    "# Complete Gradio RAG app\n",
    "# - No metadata filtering in chat\n",
    "# - Table-safe ingestion (optional Camelot) + OCR fallback\n",
    "# - Top-3 retrieved chunks with brief summaries (no raw node text)\n",
    "# - PDF viewing via base64 iframe (works regardless of local path)\n",
    "# - Compatible with older Gradio (removed unsupported args like height/wrap/scale)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import base64\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "import fitz  # PyMuPDF\n",
    "import faiss\n",
    "from PIL import Image, ImageOps\n",
    "import pytesseract\n",
    "\n",
    "# ---------- Optional Camelot (guarded table extraction) ----------\n",
    "USE_CAMELOT = True\n",
    "try:\n",
    "    import camelot  # pip install camelot-py; requires Ghostscript\n",
    "except Exception:\n",
    "    USE_CAMELOT = False\n",
    "\n",
    "# ---------- LlamaIndex / Embeddings / LLMs ----------\n",
    "from llama_index.core import Document, Settings, VectorStoreIndex, StorageContext\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "\n",
    "# ---------- App constants ----------\n",
    "UPLOADS_DIR = \"uploads\"\n",
    "os.makedirs(UPLOADS_DIR, exist_ok=True)\n",
    "\n",
    "STOP_PHRASES = [\n",
    "    \"thank you\", \"we appreciate your business\", \"page\", \"fax\", \"phone\", \"email\",\n",
    "    \"powered by\", \"confidential\", \"copyright\"\n",
    "]\n",
    "CURRENCY_RE = re.compile(r'[$€₹£]\\s?\\d')\n",
    "NUMBER_RE   = re.compile(r'\\d')\n",
    "DATE_RE     = re.compile(r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b', re.I)\n",
    "\n",
    "def _has_ruled_lines(page, min_lines=8):\n",
    "    try:\n",
    "        drawings = page.get_drawings()\n",
    "        lines = 0\n",
    "        for d in drawings:\n",
    "            for it in d.get(\"items\", []):\n",
    "                if it[0] == \"l\":\n",
    "                    lines += 1\n",
    "        return lines >= min_lines\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _row_informative(cells, min_chars=10):\n",
    "    s = \" \".join(cells).strip()\n",
    "    if len(s) < min_chars:\n",
    "        return False\n",
    "    low = s.lower()\n",
    "    if any(ph in low for ph in STOP_PHRASES):\n",
    "        return False\n",
    "    if CURRENCY_RE.search(s) or DATE_RE.search(s) or NUMBER_RE.search(s) or len(s) >= (min_chars * 2):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Mixed PDF Reader (OCR + guarded tables) ----------\n",
    "class MixedPDFReader:\n",
    "    \"\"\"\n",
    "    Page-level text + optional table row nodes (Camelot lattice by default, guarded).\n",
    "    OCR for image pages (no Camelot on OCR pages).\n",
    "    \"\"\"\n",
    "    def __init__(self, dpi=300, lang=\"eng\", min_chars=40, ocr_psm=6,\n",
    "                 extract_tables=True, allow_stream=False,\n",
    "                 min_table_rows=3, min_table_cols=3,\n",
    "                 max_rows_per_table=60, max_nodes_per_page=120):\n",
    "        self.dpi = dpi\n",
    "        self.lang = lang\n",
    "        self.min_chars = min_chars\n",
    "        self.ocr_psm = ocr_psm\n",
    "        self.extract_tables = extract_tables and USE_CAMELOT\n",
    "        self.allow_stream = allow_stream\n",
    "        self.min_table_rows = min_table_rows\n",
    "        self.min_table_cols = min_table_cols\n",
    "        self.max_rows_per_table = max_rows_per_table\n",
    "        self.max_nodes_per_page = max_nodes_per_page\n",
    "\n",
    "    def _has_meaningful_text(self, page, min_chars=None):\n",
    "        if min_chars is None:\n",
    "            min_chars = self.min_chars\n",
    "        txt = page.get_text(\"text\") or \"\"\n",
    "        return len(txt.strip()) >= min_chars\n",
    "\n",
    "    def _page_to_image(self, page):\n",
    "        scale = self.dpi / 72.0\n",
    "        pm = page.get_pixmap(matrix=fitz.Matrix(scale, scale))\n",
    "        mode = \"RGBA\" if pm.alpha else \"RGB\"\n",
    "        img = Image.frombytes(mode, (pm.width, pm.height), pm.samples)\n",
    "        if pm.alpha:\n",
    "            img = img.convert(\"RGB\")\n",
    "        img = ImageOps.autocontrast(img)\n",
    "        return img\n",
    "\n",
    "    def _read_tables(self, pdf_path, page_no, try_stream):\n",
    "        if not self.extract_tables:\n",
    "            return []\n",
    "        tables = []\n",
    "        try:\n",
    "            t_lat = camelot.read_pdf(pdf_path, pages=str(page_no), flavor=\"lattice\")\n",
    "            if t_lat and t_lat.n > 0:\n",
    "                tables = t_lat\n",
    "            elif try_stream:\n",
    "                t_str = camelot.read_pdf(pdf_path, pages=str(page_no), flavor=\"stream\")\n",
    "                if t_str and t_str.n > 0:\n",
    "                    tables = t_str\n",
    "        except Exception:\n",
    "            return []\n",
    "        return tables\n",
    "\n",
    "    def _table_is_valid(self, table):\n",
    "        df = table.df\n",
    "        n_rows, n_cols = df.shape\n",
    "        if n_rows < self.min_table_rows or n_cols < self.min_table_cols:\n",
    "            return False\n",
    "        bad = 0\n",
    "        for r in range(n_rows):\n",
    "            cells = [str(v).strip() for v in list(df.iloc[r].values)]\n",
    "            if not _row_informative(cells):\n",
    "                bad += 1\n",
    "        return (bad / max(1, n_rows)) < 0.7\n",
    "\n",
    "    def _emit_table_rows(self, table, base_meta, table_index):\n",
    "        out = []\n",
    "        df = table.df\n",
    "        n_rows, n_cols = df.shape\n",
    "\n",
    "        def looks_like_header(cells):\n",
    "            txt = \" \".join(cells)\n",
    "            return NUMBER_RE.search(txt) is None or len(txt) < 40\n",
    "\n",
    "        headers = None\n",
    "        start_row = 0\n",
    "        if n_rows > 0:\n",
    "            first = [str(v).strip() for v in list(df.iloc[0].values)]\n",
    "            if looks_like_header(first):\n",
    "                headers = first\n",
    "                start_row = 1\n",
    "\n",
    "        emitted = 0\n",
    "        for r in range(start_row, n_rows):\n",
    "            if emitted >= self.max_rows_per_table:\n",
    "                break\n",
    "            cells = [str(v).strip() for v in list(df.iloc[r].values)]\n",
    "            if not _row_informative(cells):\n",
    "                continue\n",
    "            row_text = \"\\t\".join(c for c in cells if c)\n",
    "            if not row_text:\n",
    "                continue\n",
    "            md = {\n",
    "                **base_meta,\n",
    "                \"block_type\": \"table_row\",\n",
    "                \"table_index\": table_index,\n",
    "                \"row_index\": r,\n",
    "                \"headers\": headers,\n",
    "                \"n_cols\": int(n_cols),\n",
    "            }\n",
    "            out.append(Document(text=row_text, metadata=md))\n",
    "            emitted += 1\n",
    "        return out\n",
    "\n",
    "    def load_data(self, file, extra_info=None, **kwargs):\n",
    "        doc = fitz.open(file)\n",
    "        docs = []\n",
    "        total_nodes = 0\n",
    "\n",
    "        for i, page in enumerate(doc):\n",
    "            page_no = i + 1\n",
    "            base_meta = {\n",
    "                \"source\": os.path.abspath(file),\n",
    "                \"file_name\": os.path.basename(file),\n",
    "                \"page\": page_no,\n",
    "                **(extra_info or {}),\n",
    "            }\n",
    "\n",
    "            if self._has_meaningful_text(page):\n",
    "                text = (page.get_text(\"text\") or \"\").strip()\n",
    "                ocr_applied = False\n",
    "                docs.append(Document(text=text, metadata={**base_meta, \"ocr_applied\": ocr_applied, \"block_type\": \"page_text\"}))\n",
    "                total_nodes += 1\n",
    "\n",
    "                if self.extract_tables:\n",
    "                    ruled = _has_ruled_lines(page, min_lines=8)\n",
    "                    tables = self._read_tables(file, page_no, try_stream=(self.allow_stream and not ruled)) if (ruled or self.allow_stream) else []\n",
    "                    for t_idx, t in enumerate(tables or []):\n",
    "                        if not self._table_is_valid(t):\n",
    "                            continue\n",
    "                        rows = self._emit_table_rows(t, {**base_meta, \"ocr_applied\": ocr_applied}, t_idx)\n",
    "                        docs.extend(rows)\n",
    "                        total_nodes += len(rows)\n",
    "                        if total_nodes >= self.max_nodes_per_page * page_no:\n",
    "                            break\n",
    "            else:\n",
    "                img = self._page_to_image(page)\n",
    "                text = pytesseract.image_to_string(img, lang=self.lang, config=f\"--psm {self.ocr_psm}\").strip()\n",
    "                ocr_applied = True\n",
    "                docs.append(Document(text=text, metadata={**base_meta, \"ocr_applied\": ocr_applied, \"block_type\": \"ocr_page_text\"}))\n",
    "                total_nodes += 1\n",
    "\n",
    "        return docs\n",
    "\n",
    "# ---------- LLMs / Embeddings ----------\n",
    "llm_rag = LlamaCPP(\n",
    "    model_path=\"llama_3.1_8b_q8.gguf\",   # <-- change to your GGUF path\n",
    "    temperature=0.1,\n",
    "    context_window=4096,\n",
    "    model_kwargs={\"n_gpu_layers\": -1, \"n_batch\": 256},\n",
    "    max_new_tokens=64,\n",
    "    generate_kwargs={\"stop\": [\"\\n\", \"\\n\\n\", \"Reasoning\", \"Explanation:\"]},\n",
    "    verbose=False,\n",
    ")\n",
    "Settings.llm = llm_rag\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# ---------- QA prompt + synthesizer ----------\n",
    "QA_PROMPT = PromptTemplate(\n",
    "    \"Use only the provided context. If the answer is not present, output exactly: Not found\\n\"\n",
    "    \"Return only the answer text, one short line. Do not add explanations, notes, or extra words.\\n\\n\"\n",
    "    \"Context:\\n{context_str}\\n\\n\"\n",
    "    \"Question: {query_str}\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "resp_synth = get_response_synthesizer(response_mode=\"compact\", text_qa_template=QA_PROMPT)\n",
    "\n",
    "# ---------- Robust uploader ----------\n",
    "def save_uploads_chat(files):\n",
    "    saved = []\n",
    "    stamp_dir = os.path.join(UPLOADS_DIR, str(int(time.time())))\n",
    "    os.makedirs(stamp_dir, exist_ok=True)\n",
    "\n",
    "    def _src_path(f):\n",
    "        if isinstance(f, str) and os.path.exists(f):\n",
    "            return f\n",
    "        for attr in (\"name\", \"path\"):\n",
    "            p = getattr(f, attr, None)\n",
    "            if p and os.path.exists(p):\n",
    "                return p\n",
    "        if isinstance(f, dict):\n",
    "            for k in (\"name\", \"path\"):\n",
    "                p = f.get(k)\n",
    "                if p and os.path.exists(p):\n",
    "                    return p\n",
    "        raise ValueError(f\"Could not resolve path for uploaded file: {f}\")\n",
    "\n",
    "    for f in files:\n",
    "        src = _src_path(f)\n",
    "        dst = os.path.join(stamp_dir, os.path.basename(src))\n",
    "        shutil.copy2(src, dst)\n",
    "        saved.append(dst)\n",
    "    return saved\n",
    "\n",
    "# ---------- Build logical docs (fast: one per file) ----------\n",
    "def build_logical_documents_fast(page_docs):\n",
    "    by_file = {}\n",
    "    for d in page_docs:\n",
    "        key = (d.metadata.get(\"source\"), d.metadata.get(\"file_name\"))\n",
    "        by_file.setdefault(key, []).append(d)\n",
    "    logical_docs = []\n",
    "    for (src, fname), pages in by_file.items():\n",
    "        pages.sort(key=lambda x: x.metadata.get(\"page\", 0))\n",
    "        text = \"\\n\\n\".join((p.text or \"\").strip() for p in pages if (p.text or \"\").strip())\n",
    "        if not text:\n",
    "            continue\n",
    "        logical_docs.append({\n",
    "            \"text\": text,\n",
    "            \"doc_type\": \"Other\",\n",
    "            \"page_start\": pages[0].metadata.get(\"page\", 1),\n",
    "            \"page_end\": pages[-1].metadata.get(\"page\", 1),\n",
    "            \"source_file\": src, \"file_name\": fname,\n",
    "        })\n",
    "    return logical_docs\n",
    "\n",
    "# ---------- Build index (chunk + semantic + FAISS) ----------\n",
    "def build_index_from_logical_docs_chat(logical_docs):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=120)\n",
    "    coarse_docs = []\n",
    "    for d in logical_docs:\n",
    "        chunks = splitter.split_text(d[\"text\"])\n",
    "        for cidx, chunk in enumerate(chunks):\n",
    "            coarse_docs.append(Document(\n",
    "                text=chunk,\n",
    "                metadata={\n",
    "                    \"doc_type\": d[\"doc_type\"],\n",
    "                    \"chunk_index\": cidx,\n",
    "                    \"page_start\": d[\"page_start\"],\n",
    "                    \"page_end\": d[\"page_end\"],\n",
    "                    \"source_file\": d.get(\"source_file\"),\n",
    "                    \"file_name\": d.get(\"file_name\"),\n",
    "                }\n",
    "            ))\n",
    "\n",
    "    semantic_splitter = SemanticSplitterNodeParser(\n",
    "        buffer_size=20, breakpoint_percentile_threshold=90, embed_model=Settings.embed_model\n",
    "    )\n",
    "    semantic_nodes = semantic_splitter.get_nodes_from_documents(coarse_docs)\n",
    "\n",
    "    dim = len(Settings.embed_model.get_text_embedding(\"probe\"))\n",
    "    faiss_index = faiss.IndexFlatL2(dim)\n",
    "    vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    index = VectorStoreIndex(semantic_nodes, storage_context=storage_context)\n",
    "    return index\n",
    "\n",
    "# ---------- RAG pipeline (no metadata filtering) ----------\n",
    "def build_rag_pipeline_chat(index, k_per_retriever=8, final_top_n=3, num_queries=1):\n",
    "    all_nodes = list(index.docstore.docs.values())\n",
    "    k_per = max(2, min(int(k_per_retriever), len(all_nodes)))\n",
    "    final_k = max(1, min(int(final_top_n), k_per))\n",
    "\n",
    "    vec = index.as_retriever(similarity_top_k=k_per)\n",
    "    bm25 = BM25Retriever.from_defaults(nodes=all_nodes, similarity_top_k=k_per)\n",
    "\n",
    "    fusion = QueryFusionRetriever(\n",
    "        retrievers=[vec, bm25],\n",
    "        llm=llm_rag,\n",
    "        similarity_top_k=k_per,\n",
    "        num_queries=num_queries,  # keep 1 for latency\n",
    "        mode=\"reciprocal_rerank\",\n",
    "    )\n",
    "\n",
    "    reranker = SentenceTransformerRerank(\n",
    "        model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\",\n",
    "        top_n=final_k,\n",
    "    )\n",
    "\n",
    "    qe = RetrieverQueryEngine.from_args(\n",
    "        retriever=fusion,\n",
    "        llm=llm_rag,\n",
    "        response_synthesizer=resp_synth,\n",
    "        node_postprocessors=[reranker],\n",
    "        verbose=False,\n",
    "    )\n",
    "    return qe\n",
    "\n",
    "# ---------- Final answer formatter ----------\n",
    "PLACEHOLDER_LINE = re.compile(r'^\\s*[_\\-\\.\\$€¥\\s]{3,}\\s*$', re.I)\n",
    "NOISY_FOOTERS = [\"Search results\", \"Source [\", \"Explanation:\", \"Note:\"]\n",
    "\n",
    "def finalize_answer_minimal(raw_text: str) -> str:\n",
    "    s = (raw_text or \"\").strip()\n",
    "    if not s:\n",
    "        return \"Not found\"\n",
    "    m = re.search(r'\\bnot\\s*found\\b', s, flags=re.I)\n",
    "    if m:\n",
    "        s = s[:m.start()].strip()\n",
    "    s = re.sub(r'\\([^)]*\\)', '', s).strip()\n",
    "    for marker in NOISY_FOOTERS:\n",
    "        if marker in s:\n",
    "            s = s.split(marker, 1)[0].strip()\n",
    "    parts = re.split(r'(?<=[.!?])\\s+|\\n+', s)\n",
    "    first = next((p.strip() for p in parts if p and p.strip()), \"\")\n",
    "    if not first or PLACEHOLDER_LINE.match(first):\n",
    "        return \"Not found\"\n",
    "    first = re.split(r'[.;]\\s*', first)[0].strip()\n",
    "    return first[:200] if first else \"Not found\"\n",
    "\n",
    "# ---------- Brief summary (no raw node text) ----------\n",
    "def brief_from_text(text: str, max_words: int = 14) -> str:\n",
    "    t = (text or \"\").strip()\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    t = re.sub(r\"[\\(\\)\\[\\]\\{\\}]\", \"\", t)\n",
    "    words = t.split()\n",
    "    brief = \" \".join(words[:max_words])\n",
    "    return (brief + \"…\") if len(words) > max_words else brief\n",
    "\n",
    "# ---------- Top-N nodes table (brief summaries) ----------\n",
    "def top_nodes_table_from_response(resp, k=3) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    try:\n",
    "        sns = getattr(resp, \"source_nodes\", []) or []\n",
    "        for rank, sn in enumerate(sns[:k], start=1):\n",
    "            node = sn.node\n",
    "            score = getattr(sn, \"score\", None)\n",
    "            txt = (node.get_text() or \"\")\n",
    "            md = node.metadata or {}\n",
    "            rows.append({\n",
    "                \"rank\": rank,\n",
    "                \"score\": round(float(score), 4) if score is not None else None,\n",
    "                \"source_file\": md.get(\"file_name\") or os.path.basename(md.get(\"source\") or md.get(\"source_file\") or \"\"),\n",
    "                \"doc_type\": md.get(\"doc_type\", \"\"),\n",
    "                \"page_hint\": md.get(\"page\") or md.get(\"page_start\") or \"\",\n",
    "                \"brief\": brief_from_text(txt, max_words=14),\n",
    "            })\n",
    "    except Exception as e:\n",
    "        rows.append({\"rank\": None, \"score\": None, \"source_file\": \"\", \"doc_type\": \"\", \"page_hint\": \"\", \"brief\": f\"(error building table: {e})\"})\n",
    "    return pd.DataFrame(rows, columns=[\"rank\",\"score\",\"source_file\",\"doc_type\",\"page_hint\",\"brief\"])\n",
    "\n",
    "# ---------- Build index action ----------\n",
    "def build_index_action_chat(files, progress):\n",
    "    try:\n",
    "        progress(0, desc=\"Saving uploads…\")\n",
    "        paths = save_uploads_chat(files)\n",
    "        if not paths:\n",
    "            return None, [], [], \"Please upload at least one PDF.\"\n",
    "\n",
    "        progress(0.1, desc=\"Reading PDFs (OCR / tables guarded)…\")\n",
    "        reader = MixedPDFReader(dpi=300, lang=\"eng\", min_chars=40, ocr_psm=6,\n",
    "                                extract_tables=True, allow_stream=False)\n",
    "        page_docs = []\n",
    "        for p in paths:\n",
    "            page_docs.extend(reader.load_data(p, extra_info={}))\n",
    "\n",
    "        if not page_docs:\n",
    "            return None, [], [], \"No pages were read from the PDFs.\"\n",
    "\n",
    "        progress(0.35, desc=\"Forming logical documents…\")\n",
    "        logical_docs = build_logical_documents_fast(page_docs)\n",
    "        if not logical_docs:\n",
    "            return None, [], [], \"No logical documents were formed.\"\n",
    "\n",
    "        progress(0.55, desc=\"Chunking + semantic refinement + FAISS…\")\n",
    "        index = build_index_from_logical_docs_chat(logical_docs)\n",
    "\n",
    "        msg = f\"✅ Indexed {len(logical_docs)} logical docs, {len(index.docstore.docs)} chunks.\"\n",
    "        progress(0.95, desc=\"Finalizing…\")\n",
    "        return index, logical_docs, paths, msg\n",
    "    except Exception as e:\n",
    "        return None, [], [], f\"❌ Build failed: {type(e).__name__}: {e}\"\n",
    "\n",
    "# ---------- Chat handlers ----------\n",
    "def chat_answer_chat(query, index, chat_history, k_slider):\n",
    "    chat_history = chat_history or []\n",
    "    if index is None:\n",
    "        return chat_history + [\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "            {\"role\": \"assistant\", \"content\": \"Please upload files and build the index first.\"}\n",
    "        ], pd.DataFrame()\n",
    "    qe = build_rag_pipeline_chat(\n",
    "        index,\n",
    "        k_per_retriever=int(k_slider),\n",
    "        final_top_n=min(3, int(k_slider)),\n",
    "        num_queries=1,\n",
    "    )\n",
    "    resp = qe.query(query)\n",
    "    answer = finalize_answer_minimal(str(resp).strip())\n",
    "    top_df = top_nodes_table_from_response(resp, k=3)\n",
    "    new_hist = chat_history + [{\"role\": \"user\", \"content\": query}, {\"role\": \"assistant\", \"content\": answer}]\n",
    "    return new_hist, top_df\n",
    "\n",
    "def pre_send_chat(q, hist):\n",
    "    hist = hist or []\n",
    "    q = (q or \"\").strip()\n",
    "    if not q:\n",
    "        return hist, \"\"\n",
    "    new_hist = hist + [{\"role\": \"user\", \"content\": q}, {\"role\": \"assistant\", \"content\": \"⏳ Working…\"}]\n",
    "    return new_hist, \"\"\n",
    "\n",
    "def run_answer_chat(idx, hist, k):\n",
    "    hist = hist or []\n",
    "    q = next((m[\"content\"] for m in reversed(hist) if m.get(\"role\") == \"user\"), \"\").strip()\n",
    "    if idx is None:\n",
    "        if hist and hist[-1].get(\"role\") == \"assistant\":\n",
    "            hist[-1] = {\"role\": \"assistant\", \"content\": \"Please upload files and build the index first.\"}\n",
    "        return hist, pd.DataFrame()\n",
    "    qe = build_rag_pipeline_chat(\n",
    "        idx,\n",
    "        k_per_retriever=int(k),\n",
    "        final_top_n=min(3, int(k)),\n",
    "        num_queries=1,\n",
    "    )\n",
    "    resp = qe.query(q)\n",
    "    final_text = finalize_answer_minimal(str(resp))\n",
    "    top_df = top_nodes_table_from_response(resp, k=3)\n",
    "    if hist and hist[-1].get(\"role\") == \"assistant\":\n",
    "        hist[-1] = {\"role\": \"assistant\", \"content\": final_text}\n",
    "    else:\n",
    "        hist += [{\"role\": \"assistant\", \"content\": final_text}]\n",
    "    return hist, top_df\n",
    "\n",
    "# ---------- PDF viewer (base64 iframe) ----------\n",
    "def _pdf_data_uri(path: str) -> str:\n",
    "    with open(path, \"rb\") as f:\n",
    "        b64 = base64.b64encode(f.read()).decode(\"ascii\")\n",
    "    return (\n",
    "        f'<iframe src=\"data:application/pdf;base64,{b64}#view=FitH&toolbar=1\" '\n",
    "        f'width=\"100%\" height=\"720\" style=\"border:none;\"></iframe>'\n",
    "    )\n",
    "\n",
    "def pdf_choices_from_paths(paths):\n",
    "    return [os.path.basename(p) for p in paths]\n",
    "\n",
    "def on_pdf_select(choice, paths):\n",
    "    if not choice or not paths:\n",
    "        return gr.update(value=\"\", visible=False)\n",
    "    p = next((p for p in paths if os.path.basename(p) == choice), None)\n",
    "    if not p or not os.path.exists(p):\n",
    "        return gr.update(value=\"\", visible=False)\n",
    "    html = _pdf_data_uri(p)\n",
    "    return gr.update(value=html, visible=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "94721441-62bb-4cf3-af00-483ed2fbcb7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0949308-8da0-4f60-bd5e-e2d2359fd009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-27 15:52:55,393 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-09-27 15:52:55,543 - INFO - HTTP Request: GET http://127.0.0.1:7873/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-09-27 15:52:55,554 - INFO - HTTP Request: HEAD http://127.0.0.1:7873/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7873\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7873/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-27 15:57:49,891 - DEBUG - Building index from IDs objects\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd06448c431f4b909c00a658504b3c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-27 16:00:25,864 - DEBUG - Building index from IDs objects\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174836eda1024ceca5025a96e83630a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/anubh/miniconda3/envs/llama/lib/python3.10/site-packages/gradio/queueing.py\", line 745, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/anubh/miniconda3/envs/llama/lib/python3.10/site-packages/gradio/route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/anubh/miniconda3/envs/llama/lib/python3.10/site-packages/gradio/blocks.py\", line 2112, in process_api\n",
      "    inputs = await self.preprocess_data(\n",
      "  File \"/home/anubh/miniconda3/envs/llama/lib/python3.10/site-packages/gradio/blocks.py\", line 1774, in preprocess_data\n",
      "    processed_input.append(block.preprocess(inputs_cached))\n",
      "  File \"/home/anubh/miniconda3/envs/llama/lib/python3.10/site-packages/gradio/components/dropdown.py\", line 206, in preprocess\n",
      "    raise Error(\n",
      "gradio.exceptions.Error: \"Value: blob_scanned_id_pay_return.pdf is not in the list of choices: ['LenderFeesWorksheetNew.pdf']\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anubh/miniconda3/envs/llama/lib/python3.10/site-packages/gradio/queueing.py\", line 745, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/anubh/miniconda3/envs/llama/lib/python3.10/site-packages/gradio/route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/anubh/miniconda3/envs/llama/lib/python3.10/site-packages/gradio/blocks.py\", line 2112, in process_api\n",
      "    inputs = await self.preprocess_data(\n",
      "  File \"/home/anubh/miniconda3/envs/llama/lib/python3.10/site-packages/gradio/blocks.py\", line 1774, in preprocess_data\n",
      "    processed_input.append(block.preprocess(inputs_cached))\n",
      "  File \"/home/anubh/miniconda3/envs/llama/lib/python3.10/site-packages/gradio/components/dropdown.py\", line 206, in preprocess\n",
      "    raise Error(\n",
      "gradio.exceptions.Error: \"Value: blob_scanned_id_pay_return.pdf is not in the list of choices: ['LenderFeesWorksheetNew.pdf']\"\n",
      "2025-09-27 16:15:04,432 - DEBUG - Building index from IDs objects\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc842ed806b43f192958670cdfdf104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-27 16:18:08,996 - DEBUG - Building index from IDs objects\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0ca7c4f0d14cc8bfa23ce3176cbc1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-27 16:22:52,567 - DEBUG - Building index from IDs objects\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095bf27043c8424488d14298faf261c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-27 16:25:03,837 - DEBUG - Building index from IDs objects\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc422b848064163bc5a25cc4d072102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/anubh/miniconda3/envs/llama/lib/python3.10/site-packages/gradio/queueing.py\", line 745, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/anubh/miniconda3/envs/llama/lib/python3.10/site-packages/gradio/route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/anubh/miniconda3/envs/llama/lib/python3.10/site-packages/gradio/blocks.py\", line 2112, in process_api\n",
      "    inputs = await self.preprocess_data(\n",
      "  File \"/home/anubh/miniconda3/envs/llama/lib/python3.10/site-packages/gradio/blocks.py\", line 1774, in preprocess_data\n",
      "    processed_input.append(block.preprocess(inputs_cached))\n",
      "  File \"/home/anubh/miniconda3/envs/llama/lib/python3.10/site-packages/gradio/components/dropdown.py\", line 206, in preprocess\n",
      "    raise Error(\n",
      "gradio.exceptions.Error: \"Value: blob_scanned_id_pay_return.pdf is not in the list of choices: ['LenderFeesWorksheetNew.pdf', 'LenderFeesWorksheetNew.pdf']\"\n",
      "2025-09-27 16:31:29,513 - DEBUG - Building index from IDs objects\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505e053be1cb43fd8a64fec3ad4f6c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-27 16:34:26,127 - DEBUG - Building index from IDs objects\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84c153dc133448bacb9565a1bb33e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-27 16:36:43,486 - DEBUG - Building index from IDs objects\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2589fdc5031c4aea9c738e37c8346f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-27 16:40:42,614 - DEBUG - Building index from IDs objects\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b60d3ec875d485fb2d24983b01c09fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-27 16:47:29,333 - DEBUG - Building index from IDs objects\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e16b3b4a587428d8f645581d476dcb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-27 16:49:29,419 - DEBUG - Building index from IDs objects\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc383c3be5d48059d4f0c8866bbe11b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-27 16:54:08,292 - DEBUG - Building index from IDs objects\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bfbfe5cb464ac4bca8e06dd325f389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-27 16:57:43,182 - DEBUG - Building index from IDs objects\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991dea7376404a7282ba6f86325464cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CSS = \"\"\"\n",
    ".btn-sm button { padding: 6px 12px !important; min-height: 36px !important; }\n",
    ".query-box textarea { min-height: 120px !important; font-size: 16px; }\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks(title=\"PDF RAG APP\", css=CSS) as demo:\n",
    "    gr.Markdown(\"### Local PDF RAG\\nUpload PDFs → Build index → Ask questions. Shows top‑3 retrieved chunks with brief summaries and an inline PDF viewer.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        uploader = gr.Files(label=\"Upload PDFs\", file_types=[\".pdf\"], file_count=\"multiple\")\n",
    "\n",
    "    with gr.Row():\n",
    "        build_btn = gr.Button(\"Build / Refresh Index\", variant=\"primary\")\n",
    "        clear_idx_btn = gr.Button(\"Clear Index\")\n",
    "    status = gr.Markdown()\n",
    "\n",
    "    # States\n",
    "    state_index = gr.State(None)\n",
    "    state_logical = gr.State([])\n",
    "    state_pdf_paths = gr.State([])\n",
    "\n",
    "    with gr.Row():\n",
    "        k_slider = gr.Slider(minimum=2, maximum=12, value=8, step=1, label=\"Retrieve top‑k\", interactive=True)\n",
    "\n",
    "    with gr.Row():\n",
    "        chat = gr.Chatbot(label=\"Chat\", height=360, type=\"messages\")\n",
    "\n",
    "    with gr.Row():\n",
    "        query_box = gr.Textbox(label=\"Ask a question\", placeholder=\"Ask about your documents…\",\n",
    "                               lines=5, elem_classes=[\"query-box\"])\n",
    "        with gr.Column(min_width=140):\n",
    "            send_btn = gr.Button(\"Send\", variant=\"primary\", elem_classes=[\"btn-sm\"])\n",
    "            clear_chat_btn = gr.Button(\"Clear Chat\", variant=\"secondary\", elem_classes=[\"btn-sm\"])\n",
    "\n",
    "    top_nodes_df = gr.Dataframe(\n",
    "        headers=[\"rank\",\"score\",\"source_file\",\"doc_type\",\"page_hint\",\"brief\"],\n",
    "        label=\"Top retrieved (k=3)\",\n",
    "        interactive=False\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        pdf_dropdown = gr.Dropdown(choices=[], label=\"Open a PDF\", interactive=True)\n",
    "        pdf_view = gr.HTML(label=\"PDF Viewer\", visible=False)\n",
    "\n",
    "    # Build actions\n",
    "    def on_build(files):\n",
    "        if not files:\n",
    "            return None, [], [], \"Please upload at least one PDF.\"\n",
    "        return build_index_action_chat(files, progress=gr.Progress())\n",
    "\n",
    "    def on_clear_index():\n",
    "        return None, [], [], \"Index cleared.\"\n",
    "\n",
    "    build_btn.click(\n",
    "        on_build,\n",
    "        inputs=[uploader],\n",
    "        outputs=[state_index, state_logical, state_pdf_paths, status]\n",
    "    ).then(\n",
    "        lambda paths: gr.update(choices=pdf_choices_from_paths(paths)),\n",
    "        inputs=[state_pdf_paths],\n",
    "        outputs=[pdf_dropdown],\n",
    "    ).then(\n",
    "        on_pdf_select,\n",
    "        inputs=[pdf_dropdown, state_pdf_paths],\n",
    "        outputs=[pdf_view],\n",
    "    )\n",
    "\n",
    "    clear_idx_btn.click(\n",
    "        on_clear_index,\n",
    "        outputs=[state_index, state_logical, state_pdf_paths, status]\n",
    "    ).then(\n",
    "        lambda: gr.update(choices=[]),\n",
    "        outputs=[pdf_dropdown]\n",
    "    ).then(\n",
    "        lambda: gr.update(visible=False, value=\"\"),\n",
    "        outputs=[pdf_view]\n",
    "    )\n",
    "\n",
    "    # Chat flow (returns brief summaries table)\n",
    "    send_btn.click(\n",
    "        pre_send_chat,\n",
    "        inputs=[query_box, chat],\n",
    "        outputs=[chat, query_box],\n",
    "    ).then(\n",
    "        run_answer_chat,\n",
    "        inputs=[state_index, chat, k_slider],\n",
    "        outputs=[chat, top_nodes_df],\n",
    "    )\n",
    "\n",
    "    query_box.submit(\n",
    "        pre_send_chat,\n",
    "        inputs=[query_box, chat],\n",
    "        outputs=[chat, query_box],\n",
    "    ).then(\n",
    "        run_answer_chat,\n",
    "        inputs=[state_index, chat, k_slider],\n",
    "        outputs=[chat, top_nodes_df],\n",
    "    )\n",
    "\n",
    "    clear_chat_btn.click(lambda: [], outputs=[chat])\n",
    "\n",
    "    # PDF selection change → show iframe\n",
    "    pdf_dropdown.change(\n",
    "        on_pdf_select,\n",
    "        inputs=[pdf_dropdown, state_pdf_paths],\n",
    "        outputs=[pdf_view]\n",
    "    )\n",
    "\n",
    "demo.queue().launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22b9cd09-6cd1-4ee3-9b0a-38bcbe2f2447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5d2ef6-3432-44ed-b7a2-5ec189153373",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61656eb5-679a-48b0-a7e0-ac99b8e76300",
   "metadata": {},
   "source": [
    "We delivered a local, privacy‑preserving PDF question‑answering assistant powered by a robust RAG pipeline and a minimal Gradio chat UI. The system ingests user‑uploaded PDFs (including scanned pages via OCR), classifies and stitches logical documents, semantically chunks them, indexes with an interchangeable vector store (FAISS), and answers questions with hybrid retrieval (vector + BM25), query expansion, cross‑encoder reranking, and strict output controls. \n",
    "\n",
    "The Gradio chat app stays simple: upload → build index → ask → get a concise answer with clickable source‑page previews, designed with the focus on needs of end users who would prefer simple direct answers than a complex app which has high level control over RAG. This aligns with the industry‑standard RAG pattern for grounding LLM outputs in enterprise data, and follows best‑practice retrieval steps (hybrid search, reranking, and prompt discipline) that improve accuracy and trust.\n",
    "\n",
    "It is to be noted that we could improve the performance of RAG and reduce the latency of retrievel in Gradio APP and in overall RAG pipeline by using a better LLM like gemini or running LLMs locally on machine with better resources. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
